\documentclass[
    aps,
    pra,
    twocolumn,
    superscriptaddress,
    amsmath,
    amssymb,
    floatfix
    ]{revtex4-1} 
\usepackage{graphicx} 
\usepackage{hyperref} 
\usepackage{braket}
\usepackage{mathtools}
\usepackage{bm}
\usepackage[normalem]{ulem}
\usepackage{xcolor}
\usepackage{amsthm}

\newcommand{\QAOA}{\text{QAOA}^2}
\newcommand{\bz}{\bm{z}}
\newcommand{\bx}{\bm{x}}
\newcommand{\bs}{\bm{s}}
\newcommand{\bgamma}{\bm{\gamma}}
\newcommand{\bbeta}{\bm{\beta}}
\newcommand{\bbx}{\bar{\bm{x}}}
\newcommand{\bW}{W}
 
 
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\usepackage{mathtools}

 
 
\begin{document}

\title{QAOA-in-QAOA: solving large-scale MaxCut problems on small quantum machines}

\author{Zeqiao Zhou}
\thanks{This work was done when he was a research intern at JD Explore Academy, zhouzeqiao@mail.ustc.edu.cn}
\affiliation{Department of Electronic Engineering and Information Science, University of Science and Technology of China, Hefei, Anhui, China, 230027}
\affiliation{JD Explore Academy, Beijing 101111, China}

\author{Yuxuan Du}
\thanks{Corresponding author, duyuxuan123@gmail.com}
\affiliation{JD Explore Academy, Beijing 101111, China}

\author{Xinmei Tian}
\thanks{Corresponding author, xinmei@ustc.edu.cn}
\affiliation{Department of Electronic Engineering and Information Science, University of Science and Technology of China, Hefei, Anhui, China, 230027}
 
\author{Dacheng Tao}
\thanks{Corresponding author, dacheng.tao@sydney.edu.au}
\affiliation{JD Explore Academy, Beijing 101111, China}

 
\date{\today}

\begin{abstract}
The design of fast algorithms for combinatorial optimization greatly contributes to a plethora of domains such as logistics, finance, and chemistry.  Quantum approximate optimization algorithms (QAOAs), which utilize the power of quantum machines and inherit the spirit of adiabatic evolution, are novel approaches to tackle combinatorial problems with potential runtime speedups. However, hurdled by the limited quantum resources nowadays, QAOAs are infeasible to manipulate large-scale problems. To address this issue, here we revisit the MaxCut problem via the divide-and-conquer heuristic: seek the solutions of subgraphs in parallel and then merge these solutions to obtain the global solution. Due to the $\mathbb{Z}_2$ symmetry in MaxCut, we prove that the merging process can be further cast into a new MaxCut problem and thus be addressed by QAOAs or other MaxCut solvers. With this regard, we propose QAOA-in-QAOA ($\QAOA$) to solve arbitrary large-scale MaxCut problems using small quantum machines. We also prove that the approximation ratio of $\QAOA$ is lower bounded by $1/2$. Experiment results illustrate that under different graph settings, $\QAOA$ attains a competitive or even better performance over the best known classical algorithms when the node count is around $2000$. Our method can be seamlessly embedded into other advanced strategies to enhance the capability of QAOAs in large-scale combinatorial optimization problems.
\end{abstract}

\maketitle

 
\section{Introduction}
Combinatorial optimization \cite{korteCombinatorialOptimizationTheory2006}, which aims to search for maxima or minima of an objective function with discrete solution space, is an indispensable tool in various application domains such as portfolio investment, vehicle routing and transportation \cite{juarnaCombinatorialAlgorithmsPortfolio2017,sbihiCombinatorialOptimizationGreen2007}. Driven by its fundamental importance, huge efforts have been dedicated to devising efficient algorithms for combinatorial problems during past decades. Representative examples include using semidefinite programming techniques to approximate the solution of MaxCut and maximum 2-satisfiability problems \cite{goemans879approximationAlgorithmsMAX1994,goemansImprovedApproximationAlgorithms1995},  adopting the simulated annealing methods to solve constrained problems   \cite{kirkpatrickOptimizationSimulatedAnnealing1983},  and exploiting other heuristics such as expert knowledge and the structure of studied problems to solve traveling salesman problems \cite{johnsonLocalOptimizationTraveling1990,jungerChapterTravelingSalesman1995,regoTravelingSalesmanProblem2011}. Recently, attributed to the power of neural networks, deep learning techniques have also been employed in solving combinatorial optimization problems  \cite{bengioMachineLearningCombinatorial2020,mazyavkinaReinforcementLearningCombinatorial2021,cappartCombinatorialOptimizationReasoning2021}.  Nevertheless, due to the intrinsic hardness of most combinatorial  problems \cite{karpReducibilityCombinatorialProblems1972}, existing methods request expensive computational overhead to estimate the optimal solution and thus it is highly demanded to design more powerful algorithms to speed up the optimization.

Quantum computers have the ability to efficiently solve certain problems that are computationally hard for classical computers \cite{shorAlgorithmsQuantumComputation1994}. This superiority could be preserved for noisy intermediate-scale quantum (NISQ) machines~\cite{preskillQuantumComputingNISQ2018}, because both theoretical and experimental studies have exhibited their runtime merits over the classical counterparts for certain tasks   \cite{zhongQuantumComputationalAdvantage2020,aruteQuantumSupremacyUsing2019,wuStrongQuantumComputational2021}. For this reason, there is a growing interest in designing NISQ algorithms with computational merits. Variational quantum algorithms (VQAs) \cite{cerezoVariationalQuantumAlgorithms2021}, which consist of parameterized quantum circuits \cite{benedettiParameterizedQuantumCircuits2019} and classical optimizers to adequately leverage  accessible quantum resources and suppress the system noise, are leading solutions to achieve this goal. Notably, initial studies have exhibited that quantum approximate optimization algorithms (QAOAs)~\cite{edwardfarhiQuantumApproximateOptimization2014}, as one crucial paradigm of VQAs, can be used to tackle combinatorial optimization with potential computational advantages \cite{farhiQuantumSupremacyQuantum2019a,guerreschiQAOAMaxCutRequires2019}. The underlying principle of QAOAs is mapping a quadratic unconstrained binary optimization (QUBO) problem describing the explored combinatorial problem to a Hamiltonian whose ground state refers to the optimal solution   \cite{lucasIsingFormulationsMany2014,gloverTutorialFormulatingUsing2019}. In this way,  various manipulable quantum systems can be used to advance combinatorial problems \cite{hamerlyExperimentalInvestigationPerformance2019,paganoQuantumApproximateOptimization2020,harriganQuantumApproximateOptimization2021a}.

Envisioned by the promising prospects, both empirical and theoretical studies have been carried out to understand the foundation of QAOAs and improve their performance. One critical line of research is unveiling the connection between adiabatic quantum computation   \cite{farhiQuantumComputationAdiabatic2000,farhiQuantumAdiabaticEvolution2001} and QAOAs and showing that QAOAs can be seen as a parameterized Trotterization of adiabatic evolution~\cite{zhouQuantumApproximateOptimization2020,edwardfarhiQuantumApproximateOptimization2014,wurtzCounterdiabaticityQuantumApproximate2021}.  Making use of this relation, the parameter  initialization of QAOAs can be simplified associated with an improved performance  \cite{brandaoFixedControlParameters2018,yuQuantumApproximateOptimization2021,wurtzCounterdiabaticityQuantumApproximate2021}. In parallel to explore the initialization strategy, another crucial topic is designing advanced training strategies of QAOA to avoid local optima and accelerate optimization. Concrete examples include modifying the objective functions \cite{barkoutsosImprovingVariationalQuantum2020}, applying the iterative training strategy \cite{bravyiObstaclesStatePreparation2020,camposTrainingSaturationLayerwise2021}, and using adaptive mixing operators \cite{zhuAdaptiveQuantumApproximate2020,yuQuantumApproximateOptimization2021,hadfieldQuantumApproximateOptimization2019,wangXYMixersAnalytical2020}. Despite the remarkable achievements, little progress has been made in overcoming the scalability issue of QAOAs, whereas the ultimate goal of the most advanced QAOA is solving a problem with hundreds of vertices \cite{ebadiQuantumOptimizationMaximum2022}. The main challenges come from the fact that manipulating a graph with $n$-nodes requires $O(n)$ qubits but the most advanced quantum machines nowadays can only provide a very limited number of qubits with $n\approx 100$. Moreover, due to the a high-level noise and barren plateaus, QAOAs may suffer from the trainability issue for the large $n$  \cite{lotshawScalingQuantumApproximate2022,mccleanBarrenPlateausQuantum2018,leeProgressFavorableLandscapes2021,du2021learnability,zhang2021toward}, which degrade their performance~\cite{harriganQuantumApproximateOptimization2021a,marshall2020characterizing}. Although an initial attempt of the scalable QAOAs has been addressed by   \cite{liLargescaleQuantumApproximate2021,akshay2021parameter}, their approach encounters the sample complexity issue \footnote{The approach  proposed by  \cite{liLargescaleQuantumApproximate2021} breaks one graph into two subgraphs sharing common nodes. To sample a good candidate solution, the local solution of these common nodes should be exactly overlapped. In this respect, the sample complexity of their approach grows with number of common nodes which makes it harder to sample a good candidate solution.}. To this end, it still remains obscure whether QAOAs can outperform classical approaches towards large-scale combinatorial problems. 

\begin{figure}[t]
\includegraphics[width=0.48\textwidth]{fig1.pdf}
\caption{\label{fig:1} \small{\textbf{MaxCut and QAOA} (a) One instance of MaxCut with 100 vertices. The left part is the problem graph. In the right part, two subsets of vertices are distinguished by its color as blue or red. (b) For a $p$-level QAOA,   $U_C(\gamma_i)$ and $U_B(\beta_i)$ are alternately applied to an initial state. The classical optimizer uses the measured bitstring to updated parameters of the circuit.}}
\end{figure}


To further enhance the capability of QAOAs, here we investigate the possibility of QAOAs toward solving large-scale MaxCut problems, especially when the problem size is greatly larger than the qubit count of the accessible quantum machines. In particular, we  revisit MaxCut through the lens of the divide-and-conquer heuristic, i.e., splitting the given graph into multiple subgraphs, seeking the solutions of these subgraphs in parallel,  and then merging these solutions to obtain the global solution. Notably, we prove that due to the inherent $\mathbb{Z}_2$ symmetry in MaxCut, the merging process can be cast to a new MaxCut problem. This observation hints that the large-scale MaxCut problem can be tackled by MaxCut solvers in a hierarchical way. To this end, we propose QAOA-in-QAOA  ($\QAOA$) to solve MaxCut with tens of thousands of nodes using NISQ machines. In addition, $\QAOA$ can integrate with the community detection method in the process of graph partition to attain better performance. On the theoretical side, we show that the lower bound of the approximation ratio of $\QAOA$ is $1/2$. On the experimental side, we first approximate the solution of MaxCut instances with 2000 vertices using $\QAOA$ executed on a 10-qubit quantum simulator. The achieved results are competitive or even better than the best known classical method. Moreover, through a systematical investigation, we verify that the density of graphs and subgraphs, the size of subgraphs and partition strategies are the decisive factors effecting the performance of $\QAOA$. These results suggest that $\QAOA$ provides a potential and novel way of utilizing  NISQ machines to solve practical learning problems with computational advantages.  

\begin{figure*}[htb]
\includegraphics[width=0.98\textwidth]{fig2.pdf}
\caption{\small{\textbf{Scheme of $\QAOA$.} (a) A graph is partitioned into four parts, where each one is no larger than the available number of qubits (e.g., $4$ qubits). Then the adopted MaxCut solvers are applied to all subgraphs in parallel. The green node refers to the bit $+1$ and the orange node refers to the bit $-1$. The cut edges are in red. (b) The first step highlighted by the brown arrow refers to merge local solutions of all subgraphs and calculate the value of cut between subgraphs. Yellow lines refer to `cut' and blue lines refer to `uncut'. The lower left plot indicates that the accommodation of local solutions can be reformulated a new Maxcut problem with four nodes. The first step highlighted by the brown arrow means that MaxCut solvers are applied to solve this new problem. Then, local solutions are merged according to solution of $s$. (c) An extremely large MaxCut can be solved by applying $\QAOA$ in a hierarchy way. When the graph size is above the limitation of qubits, it is partitioned and solved locally, and then reformed as a new MaxCut until its size is no larger than the available number of qubits. After all optimizations, low-level local solutions are merged according to high-level solutions.} }
\label{fig:2}
\end{figure*}

\section{Preliminaries}
The main focus of this study is solving MaxCut problems as shown in Fig.~\ref{fig:1}(a). Formally, let $\mathsf{G}(\mathsf{V}, \mathsf{E})$ be an undirected graph, where the number of vertices is $|\mathsf{V}|=N$ and the edge weight for $(i,j)\in \mathsf{E}$ is  $\bW_{ij}=W_{ji}$. Define a cut as a partition of the original set $\mathsf{V}$ into two subsets  $\mathsf{S}$ and $\mathsf{T}$ with $\mathsf{V}=\mathsf{S}\bigcup \mathsf{T}$ and $\mathsf{S}\bigcap \mathsf{T} = \emptyset$. The aim of MaxCut is to find $\mathsf{S}$ maximizing the sum of edge weight connecting vertices in $\mathsf{S}$ and $\mathsf{T}$. Denote $\bz_i=+1$ ($\bz_i=-1$)  when the $i$-th vertex is in $\mathsf{S}$   ($\mathsf{T}$), any partition of $\mathsf{V}$ can be represented by a bitstring $\bz \in \{+1,-1\}^n$. The optimal solution $\bz^*$ of MaxCut  maximizes the following objective function
\begin{equation}\label{eqn:def_maxcut}    
        C(\bz) := \frac{1}{2}\sum_{(i,j)\in \mathsf{E}} \bW_{ij}(1-\bz_i\bz_j) = c - \frac{1}{2}\sum_{(i,j)\in \mathsf{E}}\bW_{ij}\bz_i\bz_j,
\end{equation}
where $c=\frac{1}{2}\sum_{(i,j)\in \mathsf{E}} W_{ij}$ only depends on the problem and is independent of $\bm{z}$. Theoretical studies have proven that finding $\bz^*$ is NP-hard so in most cases we are searching for an approximation of $\bz^*$ \cite{karpReducibilityCombinatorialProblems1972}.  The best-known classical MaxCut solver is Goemans-Williamson (GW) algorithm \cite{goemansImprovedApproximationAlgorithms1995}, which uses the semi-definite programming (SDP) technique to ensure 0.879 approximation ratio \cite{goemans879approximationAlgorithmsMAX1994}. 

To carry out combinatorial problems on physical systems, it is necessary to   map the problem to a Hamiltonian whose ground state corresponds to the optimal solution \cite{gloverTutorialFormulatingUsing2019}. In the task of MaxCut with $n$ vertices, this Hamiltonian yields 
\begin{equation}
    \label{eq:2}
    H_C = \sum_{(i,j)\in \mathsf{E}}W_{ij}Z_iZ_j,
\end{equation}
where $Z_i$ refers to the Pauli-Z operator applied on the $i$-th qubit with $i\in[n]$ \cite{nielsenQuantumComputationQuantum2010}. The optimal solution $\ket{\bz^*}$ amounts to the computational basis state $|\bz\rangle$  minimizing $\langle \bz|H_C|\bz\rangle$. Since $H_C$ is diagonal, $\ket{\bz^*}$  also refers to its ground state. 

Quantum approximate optimization algorithm and its variants (QAOAs) \cite{edwardfarhiQuantumApproximateOptimization2014}, which absorb the merits of quantum annealing \cite{farhiQuantumSupremacyQuantum2019a} and variational quantum algorithms~\cite{cerezoVariationalQuantumAlgorithms2021}, are proposed to solve combinatorial problems on NISQ machines with potential advantages. As shown in Fig.~\ref{fig:1}(b), when applied to MaxCut, QAOA approximates the ground state by an ansatz state
\begin{equation}\label{eqn:def-QAOA}
    |\Phi(\bgamma, \bbeta)\rangle =  U_B(\bbeta_p)U_C(\bgamma_p)\dots U_B(\bbeta_1)U_C(\bgamma_1)|s\rangle
\end{equation}
where $|s\rangle=(|0\rangle^{\otimes n}+|1\rangle^{\otimes n})/\sqrt{2}$ is the initial state, $\bgamma, \bbeta \in (0, 2\pi]^p$ are variational parameters, and $U_B(\beta) = \exp(-i\beta \sum_k^nX_k)$ and $U_C(\gamma) = \exp(-i\gamma H_C)$. To find $\ket{\bz^*}$, a classical optimizer is used to update $\bgamma$ and $\bbeta$ by minimizing the following objective function 
\begin{equation}
    \label{eq:4}
    C(\bgamma,\bbeta)=\langle\Phi(\bgamma, \bbeta) | H_C|\Phi(\bgamma, \bbeta)\rangle.
\end{equation}
In the optimal setting, the optimal solution yields $\ket{\bz^*}=\ket{\Phi(\bgamma^*,\bbeta^*)}$ with $\ket{\Phi(\bgamma^*,\bbeta^*)}= \arg\min C(\bgamma,\bbeta)$.  

 \begin{figure*}[htb]
\includegraphics[width=0.98\textwidth]{fig3.pdf}
\caption{ \small{\textbf{Approximation ration of $\QAOA$ and  GW over 2000-vertex graph instances under $C_{\text{SDP}}^*$.}  Numerical results on four types of graphs. There are five instances in each type. The number of vertices is fixed to  be $2000$. The x-axis represents the graph instance G1-G5 and the y-axis represents the approximation ratio. In each panel, $\QAOA$ is in green and GW is in yellow. The number beside each box is the mean value of approximation ratios in five trials, which is represented by the square symbol in each box. }}
\label{fig:3}
\end{figure*} 

\section{QAOA-in-QAOA}
Standard QAOAs require $n$ qubits to solve a MaxCut problem with $n$ vertices. This linear dependence suppresses the   power of QAOA, since the accessible quantum resources nowadays are vey limited. To assign the capability of QAOAs for solving large-scale problems, here we revisit MaxCut through the lens of the divide-and-conquer heuristic. Specifically, in the \textit{dividing} step, we partition the  given graph $\mathsf{G}$ into $h$ subgraphs $\{\mathsf{G}_i(\mathsf{V}_i,\mathsf{E}_i)\}_{i=1}^h$, where $\mathsf{V}=\bigcup_{i=1}^h \mathsf{V}_i$ and $\mathsf{V}_i\bigcap \mathsf{V}_j = \emptyset$ when $i\neq j$. An intuition of this partition process is exhibited in Fig.~\ref{fig:2}(a). Once the partition is completed,   the MaxCut solvers are exploited to seek optimal solutions of these subgraphs in parallel. We denote the optimized solutions for all $h$ subgraphs as $\{\bx_i\}_{i=1}^h$ with $\bx_i\in \{+1, -1\}^{|\mathsf{V}_i|}$ for $\forall i\in[h]$. Due to the $\mathbb{Z}_2$ symmetry in MaxCut \cite{bravyiObstaclesStatePreparation2020}, the bitstring  $\bbx_i$, which flips all bits in $\bx_i$,  also corresponds to the solution of $\mathsf{G}_i$ for $\forall i\in[h]$. In the \textit{conquering} step, as shown in Fig.~\ref{fig:2}(b), the obtained solutions of all subgraphs are merged to obtain the global solution $\bz$ of $\mathsf{G}$. Since there are two solutions for each subgraph, the total number of the possible global solutions is $2^h$, i.e.,  $\bz\in \mathcal{Z}:=\{\bx_1,\bbx_1\}\oplus \{\bx_2,\bbx_2\}... \oplus\{\bx_h,\bbx_h\}$. Taking into account the connections among $h$ subgraphs, the global solution yields
\begin{equation}\label{eqn:maxcut-divi-co}
	\hat{\bz}=\arg\max_{\bz\in \mathcal{Z}} C(\bz).
\end{equation}
The following theorem illustrates that seeking $\hat{\bz}$  can be cast into a new MaxCut problem, where the corresponding proof is provided in Appendix \ref{apx:aggregate}. 
\begin{thm}\label{thm:di-an-co-maxcut}
Suppose that the graph $\mathsf{G}$ is partitioned into $h$ subgraphs $\{\mathsf{G}_i\}_{i=1}^h$ and the optimized local solutions are $\{\bx_i\}_{i=1}^h$. To find the bitstring $\hat{\bz}$ in Eq.~(\ref{eqn:maxcut-divi-co}), let $\bs_i= +1$ (or $-1$) be the indicator of adopting $\bx_i$ (or $\bbx_i$). Then $\hat{\bz}$  is identified by $\bs=(\bs_1,\dots, \bs_h)\in \{\pm1\}^h$, i.e., 
\begin{equation}
    \label{eq:6}
    \max_{\bz\in \mathcal{Z}}C(\bz) \equiv \max_{\bs}   C^\prime(\bs)  = -\frac{1}{2}\sum_{i < j} w_{ij}^\prime \bs_i\bs_j + c,
\end{equation}
where $w_{ij}^\prime = \frac{1}{2}\sum_u \sum_v \bx_i^{(u)}\bx_j^{(v)} W_{iu,jv} $, $\bx_i^{(u)}$ is the $u$-th bit in $\bx_i$, $W_{iu,jv}$ is the weight of edge between the two nodes corresponding to $\bx_i^{(u)}$ and $\bx_j^{(v)}$, and $c=\sum_i C_i(\bx_i) + \sum_{i < j}\frac{1}{2}\sum_u \sum_v W_{iu,jv}$, $C_i(\bx_i)$ is the optimized value of cut for the subgraph $\mathsf{G}_i$. 
\end{thm}
 

 


\begin{figure}[htb]
\includegraphics[width=0.45\textwidth]{fig7.pdf}
\caption{ \small{\textbf{Approximation ration of $\QAOA$ and  GW over 2000-vertex graph instances under $C_{\text{asymp}}^*$.}  The labels follow the same meaning with those used in  Fig.~\ref{fig:3}. The only difference is the way of calculting the approximation ratio, where the denominator is replaced by $C^*_{\text{asymp}}$. }}
\label{fig:7}
\end{figure} 

The above results hint that MaxCut problems can be tackled in a hierarchical way. According to the reformulated MaxCut in the divide-and-conquer manner, we devise QAOA-in-QAOA, dubbed \textit{$\QAOA$}, which allows us to use an $n$-qubit quantum machine to solve an $N$-vertex MaxCut with $n\ll N$. The schematic of  $\QAOA$ is shown in Fig.~\ref{fig:2}. More specifically, in the partition procedure, the graph is divided into several subgraphs that are compatible with quantum devices. This step can be achieved by using random clustering, community detection,  or other advanced strategies (See Appendix \ref{apx:partition} for elaborations). The setting of $h$ is flexible, whereas the only requirement is that the size of subgraphs should be less than $n$, i.e., $|\mathsf{V}_i|\leq n$, $\forall i \in [h]$. After partitions, all subgraphs $\{\mathsf{G}_i\}_{i=1}^h$ are solved independently by QAOAs to collect $\{\bx_i\}_{i=1}^h$. Last, to obtain the estimated global solution $\hat{\bz}$,  we apply QAOAs again to optimize the merging through reformulated MaxCut according to Theorem~\ref{thm:di-an-co-maxcut}. Note that when $h>n$, the available number of qubits is insufficient to complete the merging process. As such, $\QAOA$ applies the partition procedure successively on $C^\prime(\bs)$ until the number of subgraphs is no larger than $n$. The diagram of $\QAOA$ is summarized in Fig.~\ref{fig:2}(c).

$\QAOA$ embraces two attractive theoretical advantages. First, compared with other QAOA solvers, $\QAOA$ is immune to the scalability issue. This characteristic is highly desired for NISQ machines, which provides the opportunity to attain potential quantum advantages towards large-scale combinatorial problems. Moreover, the following theorem whose proof is given in Appendix \ref{apx:proof} guarantees that the approximation ratio of $\QAOA$ is always better than the random guess. 
\begin{thm}\label{thm:low-bound}
	Following notations in Theorem \ref{thm:di-an-co-maxcut}, the estimated solution $\hat{\bz}$ output by  $\QAOA$ always satisfies 
	\begin{equation}
		C(\hat{\bz}) \geq \frac{1}{2}\sum_{(i,j)\in \mathsf{E}}W_{ij}.
	\end{equation}
\end{thm}




\section{Experiment Results}

\begin{figure*}[htb]
\includegraphics[width=0.98\textwidth]{fig4.pdf}
\caption{  \small{(a) \textbf{Influence of graph density}. In heat map, x-axis represents the degree of vertices and the y-axis represents the size of graphs. The color of points indicates the approximation ratio of corresponding graph setting. Brighter color means higher ratio. (b) \textbf{ $\QAOA$ with different qubit size}. Fix graphs to 60-vertex and 9-degree. The performance grows as the qubit size grows from 8 to 15. Five types of curves represents the level of QAOA circuit from 1-5.  }}
\label{fig:4}
\end{figure*} 


In this section, we conduct numerical simulation to evaluate the ability of $\QAOA$ towards large-scale MaxCut problems. Specifically, we first give the setup of our experiments, including hyper-parameter settings and the constructions of graphs for MaxCut.   Then we compare performance of our proposal with other MaxCut solvers. Last, we comprehend the potential factors influencing the performance of $\QAOA$. 
 
 
To fully evaluate the performance of $\QAOA$, we collect a set of graphs varying in size, degrees of nodes, and the weight of edges. For convenience, an unweighted (or weighted) $d$-regular graph with $n$ nodes is abbreviated as ``u$d$r-$n$" (or ``w$d$r-$n$"). Similarly, for the Erdos-Renyi (ER) graph whose degree of each node is randomly assigned, an unweighted (or weighted) ER graph with the average degree being $d$ and the node size being $n$ is abbreviated as ``u$d$e-$n$" (or ``w$d$e-$n$"). For the weighted graphs, their weights are integers uniformly sampled from $[0,5]$. For each setting, we generate multiple instances to obtain the statistical results of the employed MaxCut solver.

When $\QAOA$ is applied, we specify that the allowable number of qubits is $n\leq 10$ and the random partition strategy is adopted. To find the local solutions $\bx_i$ in the dividing step, we use a 1-layer QAOA (i.e., $p=1$ in Eq.~(\ref{eqn:def-QAOA})) and the number of iterations for training QAOA is set as $T=20$.  To systematically investigate the potential of $\QAOA$ compared with the classical MaxCut solvers, we employ GW algorithm as the reference. The setting of hyper-parameters of adopted MaxCut solvers is given in Appendix \ref{append:sec:sim-res}.

In all simulations, we adopt the approximation ratio as the metric to compare the performance of different MaxCut solvers. Mathematically, given a MaxCut problem, denote $C_A$ as the cuts achieved by algorithm $\mathcal{A}$, the approximation ratio yields  
\begin{equation}
    r_{\mathcal{A}} = \frac{C_\mathcal{A}}{C^*}.
\end{equation}
where $C^*$ denotes the optimal value of MaxCut. Considering that the exact optimal value $C^*$ is exponentially difficult to get for large $N$, we substitute it with the optimal value $C^*_{\text{SDP}}$ of semidefinite programming optimization step in GW. Another metric we adopt for unweighted regular graphs and unweighted ER graphs with degree $d$ is an asymptotic value derived in \cite{demboExtremalCutsSparse2017}, where the optimal value of cut is $(\frac{d}{4} + P\sqrt{\frac{d}{4}}+o(\sqrt{d}))N$ and $P = 0.7632$ is the constant in Parisi formula. In calculating approximation rate, we set  the optimal result as $C^*_{\text{asymp}} = (\frac{d}{4} + P\sqrt{\frac{d}{4}})N$. 



We first apply $\QAOA$ to 2000-vertex graphs, i.e., u100r-2000, w100r-2000, u100e-2000, and w100e-2000. The numerical results are shown in Fig.~\ref{fig:3} and Fig.~\ref{fig:7} under the measures $C^*_{\text{SDP}}$ and $C^*_{\text{asymp}}$, respectively. Each setting includes 5 graph instances. Meanwhile, for each instance, we use $5$ different random seeds to initialize parameters of $\QAOA$. In almost all instances, $\QAOA$ outperforms GW. For example, under the measure $C_{\text{SDP}}^*$, the averaged approximation ratio $\QAOA$ is higher than that of GW, except for the instance $G_3$ with u$1000$r-2000, the instance G5 with w$1000$r-2000, and the instances G2, G3, and G5 with w$1000$e-2000. Under the measure $C_{\text{asymp}}^*$, the averaged approximation ratio $\QAOA$ is higher than that of GW, except for the instance $G_3$ with u$1000$r-2000. In this  instances, the maximal differences of the approximation ratio is only $0.0001$.  Moreover, both $\QAOA$ and GW can obtain a better performance on unweighted graphs than weighted graphs.  A possible reason is the intrinsic hardness in finding optimal solution for weighted graphs. Besides, since the distribution of weighted edges is unbalanced and the graph connectivity is sparse, random partition used in $\QAOA$ is not a suitable choice, which may leave most edges remained among subgraphs. To this end, we exhibit how advanced partition methods, i.e., community detection algorithm, can further improve the power of $\QAOA$ in Appendix \ref{append:sec:sim-res}. Note that $\QAOA$ may attain a better runtime efficiency than GW, benefiting from its hierarchical scheme and the computational advantage of quantum algorithm. 



 
\medskip
We next explore the potential factors that may influence the performance of $\QAOA$.  As mentioned previously, the sparsity of graph may reduce the power of $\QAOA$. To fully understand this effect, we conduct the systematical simulations on mild-size graphs varying in the number of nodes and the graph connectivity. Specifically, we study u$d$r-$n$, w$d$r-$n$, u$d$e-$n$ and w$d$e-$n$ with $d\in [3,9]$ and $n\in [20,100]$. For each setting, we generate 10 instances and use the average approximation ratio of them to evaluate the performance of $\QAOA$. Here both $C^*_{\text{SDP}}$ and $C^*_{\text{asymp}}$ are too loose, so we use the value of cut searched by GW algorithm as $C^*$ to calculate the approximation ratio $r_{\mathcal{A}}$. As shown in Fig.~\ref{fig:4}(a), an evident observation is that $\QAOA$ prefers denser graph than sparse graph. For example, $\QAOA$ achieves an approximation ratio of $0.9488$ on u$9$r-$20$ but only $0.5836$ on u$3$r-$100$. Consequently, a reasonable conjecture is that the main contribution of cuts comes from the cuts inside subgraphs. In order to improve the performance of $\QAOA$, one possible way is making the subgraphs as dense as possible. An alternative approach is adopting a better graph partition strategy such as the community detection method discussed in Appendix \ref{apx:partition}. 

Apart from the property of graph, the hyperparameter setting of $\QAOA$, i.e., the qubit counts $n$ and the number of layers $p$, may also effect its performance. With this regard, we consider the setting of graphs with 60 vertices and 9 degree i.e. u9r-60, u9e-60, w9r-60, and w9e-60. The number of qubits $n$ is chosen in $\{8, 10, 12, 15\}$ and the level $p$ is chosen in $[1,5]$. The results are collected across 10 instances for each setting. The achieved results are shown in Fig.~\ref{fig:4}(b). Specifically, under the measure of the averaged approximation ration, a deeper level $p$ slightly contribute much improvement. A concrete example is u9e-60, where the performance of $\QAOA$ with $p=4$ is inferior to the performance of $\QAOA$ with $p=3$.  Nevertheless,  the performance $\QAOA$ can increase significantly with the larger subgraph size $n$. For example, when the level is specified to be $p=1$, the approximation ratio of $\QAOA$ is increased by 0.06   in u9r-60 and 0.1 in w9r-60 when $n$ improves from 8 to 14. These observations indicate that executing $\QAOA$ on a large quantum system contributes to a better performance. 



\section{Discussion}

In this study, we propose $\QAOA$ that utilizes the structure of graphs  and $\mathbb{Z}_2$ symmetry to solve large-scale MaxCut problems on small-scale quantum devices. We prove that a hierarchical scheme can be achieved via reformulated MaxCuts. The approximation ratio is always greater than 0.5. The numerical results show that the proposed $\QAOA$ achieves comparable performance on $2000$-vertex graphs against the best known classical algorithm. Moreover, numerical results indicates that   $\QAOA$ can attain better performance for denser graphs which are hard for conventional QAOAs.  Our work sheds light on solving large-scale problems with potential advantages in NISQ era.

There are several important future research directions. First, it is crucial to design more instance-aware partition strategies to further improve the capabilities of $\QAOA$. Furthermore, an intriguing direction is integrating $\QAOA$ with with distributed variational quantum optimization techniques  \cite{buhrmanDistributedQuantumComputing2003,cuomoDistributedQuantumComputing2020,duAcceleratingVariationalQuantum2021}, which allows us to accelerate the optimization and understand the power of $\QAOA$ on large-scale problems. Next, since the subgraphs are independent, the performance of $\QAOA$ could be enhanced by employing advanced and problem-specific local $\QAOA$ solvers \cite{zhuAdaptiveQuantumApproximate2020,wangXYMixersAnalytical2020,du2020quantum}. Last, the concept of decomposing Hamiltonian by its symmetric property used in $\QAOA$ can be generalized to boost other variational quantum algorithms. For example, in quantum chemistry, some proposals of variational eigensolvers have used the natural symmetry of some molecular to reduce the required number of  qubits \cite{bravyiTaperingQubitsSimulate2017,liuVariationalQuantumEigensolver2019,caoLargerMolecularSimulation2021}. In quantum machine learning, the concept of decomposing Hamiltonian by its symmetric property can be leveraged to design powerful Hamiltonian-based quantum neural networks with some invariant properties \cite{meyer2022exploiting,skolik2022equivariant}. In this way, these QNNs can attain better convergence and generalization \cite{Junyu2022dynamic,du2021efficient,huang2021information,abbas2020power,du2022theory}.




\newpage
% \bibliographystyle{unsrt}
% \bibliography{HQAOA}
\begin{thebibliography}{10}

\bibitem{korteCombinatorialOptimizationTheory2006}
Bernhard Korte and Jens Vygen.
\newblock {\em Combinatorial {{Optimization}}: {{Theory}} and {{Algorithms}}}.
\newblock Algorithms and {{Combinatorics}}. {Springer-Verlag}, {Berlin
  Heidelberg}, third edition, 2006.

\bibitem{juarnaCombinatorialAlgorithmsPortfolio2017}
A.~Juarna.
\newblock Combinatorial {{Algorithms}} for {{Portfolio Optimization Problems}}
  \textendash{} {{Case}} of {{Risk Moderate Investor}}.
\newblock {\em J. Phys.: Conf. Ser.}, 820:012028, March 2017.

\bibitem{sbihiCombinatorialOptimizationGreen2007}
Abdelkader Sbihi and Richard~W. Eglese.
\newblock Combinatorial optimization and {{Green Logistics}}.
\newblock {\em 4OR}, 5(2):99--116, July 2007.

\bibitem{goemans879approximationAlgorithmsMAX1994}
Michel~X. Goemans and David~P. Williamson.
\newblock .879-approximation algorithms for {{MAX CUT}} and {{MAX 2SAT}}.
\newblock In {\em Proceedings of the Twenty-Sixth Annual {{ACM}} Symposium on
  {{Theory}} of {{Computing}}}, {{STOC}} '94, pages 422--431, {New York, NY,
  USA}, May 1994. {Association for Computing Machinery}.

\bibitem{goemansImprovedApproximationAlgorithms1995}
Michel~X. Goemans and David~P. Williamson.
\newblock Improved approximation algorithms for maximum cut and satisfiability
  problems using semidefinite programming.
\newblock {\em J. ACM}, 42(6):1115--1145, November 1995.

\bibitem{kirkpatrickOptimizationSimulatedAnnealing1983}
S.~Kirkpatrick, C.~D. Gelatt, and M.~P. Vecchi.
\newblock Optimization by {{Simulated Annealing}}.
\newblock {\em Science}, 220(4598):671--680, May 1983.

\bibitem{johnsonLocalOptimizationTraveling1990}
David~S. Johnson.
\newblock Local optimization and the {{Traveling Salesman Problem}}.
\newblock In Michael~S. Paterson, editor, {\em Automata, {{Languages}} and
  {{Programming}}}, Lecture {{Notes}} in {{Computer Science}}, pages 446--461,
  {Berlin, Heidelberg}, 1990. {Springer}.

\bibitem{jungerChapterTravelingSalesman1995}
Michael J{\"u}nger, Gerhard Reinelt, and Giovanni Rinaldi.
\newblock Chapter 4 {{The}} traveling salesman problem.
\newblock In {\em Handbooks in {{Operations Research}} and {{Management
  Science}}}, volume~7 of {\em Network {{Models}}}, pages 225--330. {Elsevier},
  January 1995.

\bibitem{regoTravelingSalesmanProblem2011}
C{\'e}sar Rego, Dorabela Gamboa, Fred Glover, and Colin Osterman.
\newblock Traveling salesman problem heuristics: {{Leading}} methods,
  implementations and latest advances.
\newblock {\em European Journal of Operational Research}, 211(3):427--441, June
  2011.

\bibitem{bengioMachineLearningCombinatorial2020}
Yoshua Bengio, Andrea Lodi, and Antoine Prouvost.
\newblock Machine {{Learning}} for {{Combinatorial Optimization}}: A
  {{Methodological Tour}} d'{{Horizon}}.
\newblock {\em arXiv:1811.06128 [cs, stat]}, March 2020.

\bibitem{mazyavkinaReinforcementLearningCombinatorial2021}
Nina Mazyavkina, Sergey Sviridov, Sergei Ivanov, and Evgeny Burnaev.
\newblock Reinforcement learning for combinatorial optimization: {{A}} survey.
\newblock {\em Computers \& Operations Research}, 134:105400, October 2021.

\bibitem{cappartCombinatorialOptimizationReasoning2021}
Quentin Cappart, Didier Ch{\'e}telat, Elias Khalil, Andrea Lodi, Christopher
  Morris, and Petar Veli{\v c}kovi{\'c}.
\newblock Combinatorial optimization and reasoning with graph neural networks.
\newblock {\em arXiv:2102.09544 [cs, math, stat]}, April 2021.

\bibitem{karpReducibilityCombinatorialProblems1972}
Richard~M. Karp.
\newblock Reducibility among {{Combinatorial Problems}}.
\newblock In Raymond~E. Miller, James~W. Thatcher, and Jean~D. Bohlinger,
  editors, {\em Complexity of {{Computer Computations}}: {{Proceedings}} of a
  Symposium on the {{Complexity}} of {{Computer Computations}}, Held {{March}}
  20\textendash 22, 1972, at the {{IBM Thomas J}}. {{Watson Research Center}},
  {{Yorktown Heights}}, {{New York}}, and Sponsored by the {{Office}} of
  {{Naval Research}}, {{Mathematics Program}}, {{IBM World Trade Corporation}},
  and the {{IBM Research Mathematical Sciences Department}}}, The {{IBM
  Research Symposia Series}}, pages 85--103. {Springer US}, {Boston, MA}, 1972.

\bibitem{shorAlgorithmsQuantumComputation1994}
P.W. Shor.
\newblock Algorithms for quantum computation: Discrete logarithms and
  factoring.
\newblock In {\em Proceedings 35th {{Annual Symposium}} on {{Foundations}} of
  {{Computer Science}}}, pages 124--134, November 1994.

\bibitem{preskillQuantumComputingNISQ2018}
John Preskill.
\newblock Quantum {{Computing}} in the {{NISQ}} era and beyond.
\newblock {\em arXiv:1801.00862 [cond-mat, physics:quant-ph]}, July 2018.

\bibitem{zhongQuantumComputationalAdvantage2020}
Han-Sen Zhong, Hui Wang, Yu-Hao Deng, Ming-Cheng Chen, Li-Chao Peng, Yi-Han
  Luo, Jian Qin, Dian Wu, Xing Ding, Yi~Hu, Peng Hu, Xiao-Yan Yang, Wei-Jun
  Zhang, Hao Li, Yuxuan Li, Xiao Jiang, Lin Gan, Guangwen Yang, Lixing You,
  Zhen Wang, Li~Li, Nai-Le Liu, Chao-Yang Lu, and Jian-Wei Pan.
\newblock Quantum computational advantage using photons.
\newblock {\em Science}, 370(6523):1460, December 2020.

\bibitem{aruteQuantumSupremacyUsing2019}
Frank Arute, Kunal Arya, Ryan Babbush, Dave Bacon, Joseph~C. Bardin, Rami
  Barends, Rupak Biswas, Sergio Boixo, Fernando G. S.~L. Brandao, David~A.
  Buell, Brian Burkett, Yu~Chen, Zijun Chen, Ben Chiaro, Roberto Collins,
  William Courtney, Andrew Dunsworth, Edward Farhi, Brooks Foxen, Austin
  Fowler, Craig Gidney, Marissa Giustina, Rob Graff, Keith Guerin, Steve
  Habegger, Matthew~P. Harrigan, Michael~J. Hartmann, Alan Ho, Markus Hoffmann,
  Trent Huang, Travis~S. Humble, Sergei~V. Isakov, Evan Jeffrey, Zhang Jiang,
  Dvir Kafri, Kostyantyn Kechedzhi, Julian Kelly, Paul~V. Klimov, Sergey Knysh,
  Alexander Korotkov, Fedor Kostritsa, David Landhuis, Mike Lindmark, Erik
  Lucero, Dmitry Lyakh, Salvatore Mandr{\`a}, Jarrod~R. McClean, Matthew
  McEwen, Anthony Megrant, Xiao Mi, Kristel Michielsen, Masoud Mohseni, Josh
  Mutus, Ofer Naaman, Matthew Neeley, Charles Neill, Murphy~Yuezhen Niu, Eric
  Ostby, Andre Petukhov, John~C. Platt, Chris Quintana, Eleanor~G. Rieffel,
  Pedram Roushan, Nicholas~C. Rubin, Daniel Sank, Kevin~J. Satzinger, Vadim
  Smelyanskiy, Kevin~J. Sung, Matthew~D. Trevithick, Amit Vainsencher, Benjamin
  Villalonga, Theodore White, Z.~Jamie Yao, Ping Yeh, Adam Zalcman, Hartmut
  Neven, and John~M. Martinis.
\newblock Quantum supremacy using a programmable superconducting processor.
\newblock {\em Nature}, 574(7779):505--510, October 2019.

\bibitem{wuStrongQuantumComputational2021}
Yulin Wu, Wan-Su Bao, Sirui Cao, Fusheng Chen, Ming-Cheng Chen, Xiawei Chen,
  Tung-Hsun Chung, Hui Deng, Yajie Du, Daojin Fan, Ming Gong, Cheng Guo, Chu
  Guo, Shaojun Guo, Lianchen Han, Linyin Hong, He-Liang Huang, Yong-Heng Huo,
  Liping Li, Na~Li, Shaowei Li, Yuan Li, Futian Liang, Chun Lin, Jin Lin,
  Haoran Qian, Dan Qiao, Hao Rong, Hong Su, Lihua Sun, Liangyuan Wang, Shiyu
  Wang, Dachao Wu, Yu~Xu, Kai Yan, Weifeng Yang, Yang Yang, Yangsen Ye,
  Jianghan Yin, Chong Ying, Jiale Yu, Chen Zha, Cha Zhang, Haibin Zhang, Kaili
  Zhang, Yiming Zhang, Han Zhao, Youwei Zhao, Liang Zhou, Qingling Zhu,
  Chao-Yang Lu, Cheng-Zhi Peng, Xiaobo Zhu, and Jian-Wei Pan.
\newblock Strong quantum computational advantage using a superconducting
  quantum processor.
\newblock {\em arXiv:2106.14734 [quant-ph]}, June 2021.

\bibitem{cerezoVariationalQuantumAlgorithms2021}
M.~Cerezo, Andrew Arrasmith, Ryan Babbush, Simon~C. Benjamin, Suguru Endo,
  Keisuke Fujii, Jarrod~R. McClean, Kosuke Mitarai, Xiao Yuan, Lukasz Cincio,
  and Patrick~J. Coles.
\newblock Variational quantum algorithms.
\newblock {\em Nat Rev Phys}, 3(9):625--644, September 2021.

\bibitem{benedettiParameterizedQuantumCircuits2019}
Marcello Benedetti, Erika Lloyd, Stefan Sack, and Mattia Fiorentini.
\newblock Parameterized quantum circuits as machine learning models.
\newblock {\em Quantum Sci. Technol.}, 4(4):043001, November 2019.

\bibitem{edwardfarhiQuantumApproximateOptimization2014}
{Edward Farhi}, Jeffrey Goldstone, and Sam Gutmann.
\newblock A {{Quantum Approximate Optimization Algorithm}}.
\newblock {\em arXiv:1411.4028 [quant-ph]}, November 2014.

\bibitem{farhiQuantumSupremacyQuantum2019a}
Edward Farhi and Aram~W. Harrow.
\newblock Quantum {{Supremacy}} through the {{Quantum Approximate Optimization
  Algorithm}}.
\newblock {\em arXiv:1602.07674 [quant-ph]}, October 2019.

\bibitem{guerreschiQAOAMaxCutRequires2019}
G.~G. Guerreschi and A.~Y. Matsuura.
\newblock {{QAOA}} for {{Max-Cut}} requires hundreds of qubits for quantum
  speed-up.
\newblock {\em Sci Rep}, 9(1):6903, December 2019.

\bibitem{lucasIsingFormulationsMany2014}
Andrew Lucas.
\newblock Ising formulations of many {{NP}} problems.
\newblock {\em Front. Physics}, 2, 2014.

\bibitem{gloverTutorialFormulatingUsing2019}
Fred Glover, Gary Kochenberger, and Yu~Du.
\newblock A {{Tutorial}} on {{Formulating}} and {{Using QUBO Models}}.
\newblock {\em arXiv:1811.11538 [quant-ph]}, November 2019.

\bibitem{hamerlyExperimentalInvestigationPerformance2019}
Ryan Hamerly, Takahiro Inagaki, Peter~L. McMahon, Davide Venturelli, Alireza
  Marandi, Tatsuhiro Onodera, Edwin Ng, Carsten Langrock, Kensuke Inaba,
  Toshimori Honjo, Koji Enbutsu, Takeshi Umeki, Ryoichi Kasahara, Shoko
  Utsunomiya, Satoshi Kako, Ken-ichi Kawarabayashi, Robert~L. Byer, Martin~M.
  Fejer, Hideo Mabuchi, Dirk Englund, Eleanor Rieffel, Hiroki Takesue, and
  Yoshihisa Yamamoto.
\newblock Experimental investigation of performance differences between
  {{Coherent Ising Machines}} and a quantum annealer.
\newblock {\em Sci. Adv.}, 5(5):eaau0823, May 2019.

\bibitem{paganoQuantumApproximateOptimization2020}
G.~Pagano, A.~Bapat, P.~Becker, K.~S. Collins, A.~De, P.~W. Hess, H.~B. Kaplan,
  A.~Kyprianidis, W.~L. Tan, C.~Baldwin, L.~T. Brady, A.~Deshpande, F.~Liu,
  S.~Jordan, A.~V. Gorshkov, and C.~Monroe.
\newblock Quantum {{Approximate Optimization}} of the {{Long-Range Ising
  Model}} with a {{Trapped-Ion Quantum Simulator}}.
\newblock {\em Proc Natl Acad Sci USA}, 117(41):25396--25401, October 2020.

\bibitem{harriganQuantumApproximateOptimization2021a}
Matthew~P. Harrigan, Kevin~J. Sung, Matthew Neeley, Kevin~J. Satzinger, Frank
  Arute, Kunal Arya, Juan Atalaya, Joseph~C. Bardin, Rami Barends, Sergio
  Boixo, Michael Broughton, Bob~B. Buckley, David~A. Buell, Brian Burkett,
  Nicholas Bushnell, Yu~Chen, Zijun Chen, Ben Chiaro, Roberto Collins, William
  Courtney, Sean Demura, Andrew Dunsworth, Daniel Eppens, Austin Fowler, Brooks
  Foxen, Craig Gidney, Marissa Giustina, Rob Graff, Steve Habegger, Alan Ho,
  Sabrina Hong, Trent Huang, L.~B. Ioffe, Sergei~V. Isakov, Evan Jeffrey, Zhang
  Jiang, Cody Jones, Dvir Kafri, Kostyantyn Kechedzhi, Julian Kelly, Seon Kim,
  Paul~V. Klimov, Alexander~N. Korotkov, Fedor Kostritsa, David Landhuis, Pavel
  Laptev, Mike Lindmark, Martin Leib, Orion Martin, John~M. Martinis, Jarrod~R.
  McClean, Matt McEwen, Anthony Megrant, Xiao Mi, Masoud Mohseni, Wojciech
  Mruczkiewicz, Josh Mutus, Ofer Naaman, Charles Neill, Florian Neukart,
  Murphy~Yuezhen Niu, Thomas~E. O'Brien, Bryan O'Gorman, Eric Ostby, Andre
  Petukhov, Harald Putterman, Chris Quintana, Pedram Roushan, Nicholas~C.
  Rubin, Daniel Sank, Andrea Skolik, Vadim Smelyanskiy, Doug Strain, Michael
  Streif, Marco Szalay, Amit Vainsencher, Theodore White, Z.~Jamie Yao, Ping
  Yeh, Adam Zalcman, Leo Zhou, Hartmut Neven, Dave Bacon, Erik Lucero, Edward
  Farhi, and Ryan Babbush.
\newblock Quantum {{Approximate Optimization}} of {{Non-Planar Graph Problems}}
  on a {{Planar Superconducting Processor}}.
\newblock {\em Nat. Phys.}, 17(3):332--336, March 2021.

\bibitem{farhiQuantumComputationAdiabatic2000}
Edward Farhi, Jeffrey Goldstone, Sam Gutmann, and Michael Sipser.
\newblock Quantum {{Computation}} by {{Adiabatic Evolution}}.
\newblock {\em arXiv:quant-ph/0001106}, January 2000.

\bibitem{farhiQuantumAdiabaticEvolution2001}
Edward Farhi, Jeffrey Goldstone, Sam Gutmann, Joshua Lapan, Andrew Lundgren,
  and Daniel Preda.
\newblock A {{Quantum Adiabatic Evolution Algorithm Applied}} to {{Random
  Instances}} of an {{NP-Complete Problem}}.
\newblock {\em Science}, 292(5516):472--475, April 2001.

\bibitem{zhouQuantumApproximateOptimization2020}
Leo Zhou, Sheng-Tao Wang, Soonwon Choi, Hannes Pichler, and Mikhail~D. Lukin.
\newblock Quantum {{Approximate Optimization Algorithm}}: {{Performance}},
  {{Mechanism}}, and {{Implementation}} on {{Near-Term Devices}}.
\newblock {\em Phys. Rev. X}, 10(2):021067, June 2020.

\bibitem{wurtzCounterdiabaticityQuantumApproximate2021}
Jonathan Wurtz and Peter~J. Love.
\newblock Counterdiabaticity and the quantum approximate optimization
  algorithm.
\newblock {\em arXiv:2106.15645 [quant-ph]}, June 2021.

\bibitem{brandaoFixedControlParameters2018}
Fernando G. S.~L. Brandao, Michael Broughton, Edward Farhi, Sam Gutmann, and
  Hartmut Neven.
\newblock For {{Fixed Control Parameters}} the {{Quantum Approximate
  Optimization Algorithm}}'s {{Objective Function Value Concentrates}} for
  {{Typical Instances}}.
\newblock {\em arXiv:1812.04170 [quant-ph]}, December 2018.

\bibitem{yuQuantumApproximateOptimization2021}
Yunlong Yu, Chenfeng Cao, Carter Dewey, Xiang-Bin Wang, Nic Shannon, and Robert
  Joynt.
\newblock Quantum {{Approximate Optimization Algorithm}} with {{Adaptive Bias
  Fields}}.
\newblock {\em arXiv:2105.11946 [quant-ph]}, May 2021.

\bibitem{barkoutsosImprovingVariationalQuantum2020}
Panagiotis~Kl Barkoutsos, Giacomo Nannicini, Anton Robert, Ivano Tavernelli,
  and Stefan Woerner.
\newblock Improving {{Variational Quantum Optimization}} using {{CVaR}}.
\newblock {\em Quantum}, 4:256, April 2020.

\bibitem{bravyiObstaclesStatePreparation2020}
Sergey Bravyi, Alexander Kliesch, Robert Koenig, and Eugene Tang.
\newblock Obstacles to {{State Preparation}} and {{Variational Optimization}}
  from {{Symmetry Protection}}.
\newblock {\em Phys. Rev. Lett.}, 125(26):260505, December 2020.

\bibitem{camposTrainingSaturationLayerwise2021}
E.~Campos, D.~Rabinovich, V.~Akshay, and J.~Biamonte.
\newblock Training {{Saturation}} in {{Layerwise Quantum Approximate
  Optimisation}}.
\newblock {\em arXiv:2106.13814 [cond-mat, physics:quant-ph]}, June 2021.

\bibitem{zhuAdaptiveQuantumApproximate2020}
Linghua Zhu, Ho~Lun Tang, George~S. Barron, F.~A. {Calderon-Vargas},
  Nicholas~J. Mayhall, Edwin Barnes, and Sophia~E. Economou.
\newblock An adaptive quantum approximate optimization algorithm for solving
  combinatorial problems on a quantum computer.
\newblock {\em arXiv:2005.10258 [quant-ph]}, December 2020.

\bibitem{hadfieldQuantumApproximateOptimization2019}
Stuart Hadfield, Zhihui Wang, Bryan O'Gorman, Eleanor~G. Rieffel, Davide
  Venturelli, and Rupak Biswas.
\newblock From the {{Quantum Approximate Optimization Algorithm}} to a
  {{Quantum Alternating Operator Ansatz}}.
\newblock {\em Algorithms}, 12(2):34, February 2019.

\bibitem{wangXYMixersAnalytical2020}
Zhihui Wang, Nicholas~C. Rubin, Jason~M. Dominy, and Eleanor~G. Rieffel.
\newblock \${{XY}}\$-mixers: Analytical and numerical results for {{QAOA}}.
\newblock {\em Phys. Rev. A}, 101(1):012320, January 2020.

\bibitem{ebadiQuantumOptimizationMaximum2022}
Sepehr Ebadi, Alexander Keesling, Madelyn Cain, Tout~T. Wang, Harry Levine,
  Dolev Bluvstein, Giulia Semeghini, Ahmed Omran, Jinguo Liu, Rhine Samajdar,
  Xiu-Zhe Luo, Beatrice Nash, Xun Gao, Boaz Barak, Edward Farhi, Subir Sachdev,
  Nathan Gemelke, Leo Zhou, Soonwon Choi, Hannes Pichler, Shengtao Wang, Markus
  Greiner, Vladan Vuletic, and Mikhail~D. Lukin.
\newblock Quantum {{Optimization}} of {{Maximum Independent Set}} using
  {{Rydberg Atom Arrays}}.
\newblock {\em arXiv:2202.09372 [cond-mat, physics:physics, physics:quant-ph]},
  February 2022.

\bibitem{lotshawScalingQuantumApproximate2022}
Phillip~C. Lotshaw, Thien Nguyen, Anthony Santana, Alexander McCaskey, Rebekah
  Herrman, James Ostrowski, George Siopsis, and Travis~S. Humble.
\newblock Scaling {{Quantum Approximate Optimization}} on {{Near-term
  Hardware}}.
\newblock {\em arXiv:2201.02247 [quant-ph]}, January 2022.

\bibitem{mccleanBarrenPlateausQuantum2018}
Jarrod~R. McClean, Sergio Boixo, Vadim~N. Smelyanskiy, Ryan Babbush, and
  Hartmut Neven.
\newblock Barren plateaus in quantum neural network training landscapes.
\newblock {\em Nature Communications}, 9(1):4812, November 2018.

\bibitem{leeProgressFavorableLandscapes2021}
Juneseo Lee, Alicia~B. Magann, Herschel~A. Rabitz, and Christian Arenz.
\newblock Progress toward favorable landscapes in quantum combinatorial
  optimization.
\newblock {\em Phys. Rev. A}, 104(3):032401, September 2021.

\bibitem{du2021learnability}
Yuxuan Du, Min-Hsiu Hsieh, Tongliang Liu, Shan You, and Dacheng Tao.
\newblock Learnability of quantum neural networks.
\newblock {\em PRX Quantum}, 2(4):040337, 2021.

\bibitem{zhang2021toward}
Kaining Zhang, Min-Hsiu Hsieh, Liu Liu, and Dacheng Tao.
\newblock Toward trainability of deep quantum neural networks.
\newblock {\em arXiv preprint arXiv:2112.15002}, 2021.

\bibitem{marshall2020characterizing}
Jeffrey Marshall, Filip Wudarski, Stuart Hadfield, and Tad Hogg.
\newblock Characterizing local noise in qaoa circuits.
\newblock {\em IOP SciNotes}, 1(2):025208, 2020.

\bibitem{liLargescaleQuantumApproximate2021}
Junde Li, Mahabubul Alam, and Swaroop Ghosh.
\newblock Large-scale {{Quantum Approximate Optimization}} via
  {{Divide-and-Conquer}}.
\newblock {\em arXiv:2102.13288 [quant-ph]}, February 2021.

\bibitem{akshay2021parameter}
Vishwanathan Akshay, Daniil Rabinovich, Ernesto Campos, and Jacob Biamonte.
\newblock Parameter concentrations in quantum approximate optimization.
\newblock {\em Physical Review A}, 104(1):L010401, 2021.

\bibitem{Note1}
The approach proposed by \cite {liLargescaleQuantumApproximate2021} breaks one
  graph into two subgraphs sharing common nodes. To sample a good candidate
  solution, the local solution of these common nodes should be exactly
  overlapped. In this respect, the sample complexity of their approach grows
  with number of common nodes which makes it harder to sample a good candidate
  solution.

\bibitem{nielsenQuantumComputationQuantum2010}
Michael~A. Nielsen and Isaac~L. Chuang.
\newblock {\em Quantum {{Computation}} and {{Quantum Information}}: 10th
  {{Anniversary Edition}}}.
\newblock {Cambridge University Press}, {Cambridge}, 2010.

\bibitem{demboExtremalCutsSparse2017}
Amir Dembo, Andrea Montanari, and Subhabrata Sen.
\newblock Extremal {{Cuts}} of {{Sparse Random Graphs}}.
\newblock {\em Ann. Probab.}, 45(2), March 2017.

\bibitem{buhrmanDistributedQuantumComputing2003}
Harry Buhrman and Hein R{\"o}hrig.
\newblock Distributed {{Quantum Computing}}.
\newblock In Branislav Rovan and Peter Vojt{\'a}{\v s}, editors, {\em
  Mathematical {{Foundations}} of {{Computer Science}} 2003}, Lecture {{Notes}}
  in {{Computer Science}}, pages 1--20, {Berlin, Heidelberg}, 2003. {Springer}.

\bibitem{cuomoDistributedQuantumComputing2020}
Daniele Cuomo, Marcello Caleffi, and Angela~Sara Cacciapuoti.
\newblock Towards a {{Distributed Quantum Computing Ecosystem}}.
\newblock {\em IET Quantum Communication}, 1(1):3--8, July 2020.

\bibitem{duAcceleratingVariationalQuantum2021}
Y.~Du, Y.~Qian, X.~Wu, and D.~Tao.
\newblock Accelerating variational quantum algorithms with multiple quantum
  processors.
\newblock {\em IEEE Transactions on Quantum Engineering}, (01):1--1, may 2022.

\bibitem{du2020quantum}
Yuxuan Du, Tao Huang, Shan You, Min-Hsiu Hsieh, and Dacheng Tao.
\newblock Quantum circuit architecture search for variational quantum
  algorithms.
\newblock {\em npj Quantum Information}, 8, 2022.

\bibitem{bravyiTaperingQubitsSimulate2017}
Sergey Bravyi, Jay~M. Gambetta, Antonio Mezzacapo, and Kristan Temme.
\newblock Tapering off qubits to simulate fermionic {{Hamiltonians}}.
\newblock {\em arXiv:1701.08213 [quant-ph]}, January 2017.

\bibitem{liuVariationalQuantumEigensolver2019}
Jin-Guo Liu, Yi-Hong Zhang, Yuan Wan, and Lei Wang.
\newblock Variational {{Quantum Eigensolver}} with {{Fewer Qubits}}.
\newblock {\em Phys. Rev. Research}, 1(2):023025, September 2019.

\bibitem{caoLargerMolecularSimulation2021}
Changsu Cao, Jiaqi Hu, Wengang Zhang, Xusheng Xu, Dechin Chen, Fan Yu, Jun Li,
  Hanshi Hu, Dingshun Lv, and Man-Hong Yung.
\newblock Towards a {{Larger Molecular Simulation}} on the {{Quantum
  Computer}}: {{Up}} to 28 {{Qubits Systems Accelerated}} by {{Point Group
  Symmetry}}.
\newblock {\em arXiv:2109.02110 [quant-ph]}, September 2021.

\bibitem{meyer2022exploiting}
Johannes~Jakob Meyer, Marian Mularski, Elies Gil-Fuster, Antonio~Anna Mele,
  Francesco Arzani, Alissa Wilms, and Jens Eisert.
\newblock Exploiting symmetry in variational quantum machine learning.
\newblock {\em arXiv preprint arXiv:2205.06217}, 2022.

\bibitem{skolik2022equivariant}
Andrea Skolik, Michele Cattelan, Sheir Yarkoni, Thomas B{\"a}ck, and Vedran
  Dunjko.
\newblock Equivariant quantum circuits for learning on weighted graphs.
\newblock {\em arXiv preprint arXiv:2205.06109}, 2022.

\bibitem{Junyu2022dynamic}
Junyu Liu, Khadijeh Najafi, Kunal Sharma, Francesco Tacchino, Liang Jiang, and
  Antonio Mezzacapo.
\newblock {A}n analytic theory for the dynamics of wide quantum neural
  networks, 2022.
\newblock arXiv:2203.16711v1.

\bibitem{du2021efficient}
Yuxuan Du, Zhuozhuo Tu, Xiao Yuan, and Dacheng Tao.
\newblock Efficient measure for the expressivity of variational quantum
  algorithms.
\newblock {\em Physical Review Letters}, 128(8):080506, 2022.

\bibitem{huang2021information}
Hsin-Yuan Huang, Richard Kueng, and John Preskill.
\newblock Information-theoretic bounds on quantum advantage in machine
  learning.
\newblock {\em Physical Review Letters}, 126(19):190505, 2021.

\bibitem{abbas2020power}
Amira Abbas, David Sutter, Christa Zoufal, Aur{\'e}lien Lucchi, Alessio
  Figalli, and Stefan Woerner.
\newblock The power of quantum neural networks.
\newblock {\em Nature Computational Science}, 1(6):403--409, 2021.

\bibitem{du2022theory}
Yuxuan Du, Zhuozhuo Tu, Bujiao Wu, Xiao Yuan, and Dacheng Tao.
\newblock Theory of quantum generative learning models with maximum mean
  discrepancy.
\newblock {\em arXiv preprint arXiv:2205.04730}, 2022.

\bibitem{newmanFindingEvaluatingCommunity2004}
M.~E.~J. Newman and M.~Girvan.
\newblock Finding and evaluating community structure in networks.
\newblock {\em Phys. Rev. E}, 69(2):026113, February 2004.

\bibitem{clausetFindingCommunityStructure2004}
Aaron Clauset, M.~E.~J. Newman, and Cristopher Moore.
\newblock Finding community structure in very large networks.
\newblock {\em arXiv:cond-mat/0408187}, August 2004.

\bibitem{newmanFastAlgorithmDetecting2004}
M.~E.~J. Newman.
\newblock Fast algorithm for detecting community structure in networks.
\newblock {\em Phys. Rev. E}, 69(6):066133, June 2004.

\bibitem{blondelFastUnfoldingCommunities2008}
Vincent~D. Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne
  Lefebvre.
\newblock Fast unfolding of communities in large networks.
\newblock {\em J. Stat. Mech.}, 2008(10):P10008, October 2008.

\bibitem{gokhaleMinimizingStatePreparations2019}
Pranav Gokhale, Olivia Angiuli, Yongshan Ding, Kaiwen Gui, Teague Tomesh,
  Martin Suchara, Margaret Martonosi, and Frederic~T. Chong.
\newblock Minimizing {{State Preparations}} in {{Variational Quantum
  Eigensolver}} by {{Partitioning}} into {{Commuting Families}}.
\newblock {\em arXiv:1907.13623 [quant-ph]}, July 2019.

\bibitem{verteletskyiMeasurementOptimizationVariational2020}
Vladyslav Verteletskyi, Tzu-Ching Yen, and Artur~F. Izmaylov.
\newblock Measurement {{Optimization}} in the {{Variational Quantum Eigensolver
  Using}} a {{Minimum Clique Cover}}.
\newblock {\em J. Chem. Phys.}, 152(12):124114, March 2020.

\bibitem{zhangVariationalQuantumEigensolver2021}
Yu~Zhang, Lukasz Cincio, Christian F.~A. Negre, Piotr Czarnik, Patrick Coles,
  Petr~M. Anisimov, Susan~M. Mniszewski, Sergei Tretiak, and Pavel~A. Dub.
\newblock Variational {{Quantum Eigensolver}} with {{Reduced Circuit
  Complexity}}.
\newblock {\em arXiv:2106.07619 [quant-ph]}, June 2021.

\bibitem{bergholmPennyLaneAutomaticDifferentiation2020}
Ville Bergholm, Josh Izaac, Maria Schuld, Christian Gogolin, M.~Sohaib Alam,
  Shahnawaz Ahmed, Juan~Miguel Arrazola, Carsten Blank, Alain Delgado, Soran
  Jahangiri, Keri McKiernan, Johannes~Jakob Meyer, Zeyue Niu, Antal Sz{\'a}va,
  and Nathan Killoran.
\newblock {{PennyLane}}: {{Automatic}} differentiation of hybrid
  quantum-classical computations.
\newblock {\em arXiv:1811.04968 [physics, physics:quant-ph]}, February 2020.

\bibitem{diamondCVXPYPythonEmbeddedModeling2016}
Steven Diamond and Stephen Boyd.
\newblock {{CVXPY}}: {{A Python-Embedded Modeling Language}} for {{Convex
  Optimization}}.
\newblock {\em arXiv:1603.00943 [math]}, June 2016.

\bibitem{odonoghueConicOptimizationOperator2016}
Brendan O'Donoghue, Eric Chu, Neal Parikh, and Stephen Boyd.
\newblock Conic {{Optimization}} via {{Operator Splitting}} and {{Homogeneous
  Self-Dual Embedding}}.
\newblock {\em J Optim Theory Appl}, 169(3):1042--1068, June 2016.

\bibitem{Note2}
https://github.com/networkx/networkx.

\end{thebibliography}


\clearpage
\newpage
\appendix

\onecolumngrid
 
\begin{center}
\large{\textbf{Supplementary Material: ``QAOA-in-QAOA: solving large-scale MaxCut problems on small quantum machines''}}	
\end{center}
 
\medskip 
%\twocolumngrid
\onecolumngrid

The organization of Supplementary Material is as follows. In Appendix \ref{apx:partition}, we present how advanced partitioned methods can further improve the performance of $\QAOA$. In Appendix \ref{apx:aggregate}, we provide the proof of Theorem \ref{thm:di-an-co-maxcut}. Then in Appendix \ref{apx:proof}, we demonstrate the proof of Theorem \ref{thm:low-bound}. Subsequently, we discuss how $\QAOA$ relates to the Hamiltonian splitting method ind Appendix \ref{apx:physics}. Last, in Appendix \ref{append:sec:sim-res}, we exhibit the omitted simulation details and more simulation results of $\QAOA$. 

 
 

\section{Graph partitioning}\label{apx:partition}

A crucial step in $\QAOA$ is partitioning graph into subgraphs.  We note that the way of partition is diverse. Here we present two possible partition methods, i.e., random partition and   community detection based partition. We leave the design of more advanced partition methods as the future work.

\medskip
\textbf{Random partition.} The algorithmic implementation of the random partition is as follows. Given the number of qubits $n$ and a graph with size $N$, random partition successively samples $n$ vertices as a subgraph without replacement until all $\lceil N/n \rceil$ subgraphs are collected.


We remark that for dense graphs, random partition promises a good learning performance of $\QAOA$ since the probability of an edge existing between arbitrary two vertex is high. In contrast, for graphs whose expected vertex degree is low, random partition may lead to an inferior performance. This is because the collected subgraphs may contain few edges and most edges remaining between subgraphs.

\medskip
\textbf{Community detection based partition.} According to the above explanations, a natural idea to enhance the power of $\QAOA$ is to maintain as many edges inside each subgraph as possible, which in turn suppresses the error incurred by  partition. With this regard, we introduce modularity \cite{newmanFindingEvaluatingCommunity2004} as a measure of the quality of partitioning. A mathematical definition of modularity is 
\begin{equation}
    Q = \frac{1}{2m}\sum_{ij}\left[A_{ij}-\frac{k_ik_j}{2m}\right]\delta(c_i,c_j),
    \label{eq:a1}
\end{equation} 
where $m$ is the sum of weights of all edges, $A_{ij}$ is the element of adjacency matrix, $k_i$ is the sum of weights of edges connected to vertex $i$, and $c_i$ is the community that the vertex $i$ is assigned. Intuitively, the term $\frac{k_ik_j}{2m}$ indicates the probability of edge existing between $i$ and $j$ in a randomized graph. When the fraction of edges within communities equals to a randomized graph, the quantity $Q$ will be zero. When $Q > 0.3$, it indicates a significant community structure \cite{clausetFindingCommunityStructure2004}. 

$\QAOA$ pursues a high modularity $Q$, where the connectivity in subgraphs is dense but the connectivity between different subgraphs is sparse. An algorithm searching for high modularity partition is referred to as \textit{community detection algorithm}, which regards subgraphs as communities. Several algorithms have been proposed to maximize modularity and to find community structure in graphs \cite{newmanFindingEvaluatingCommunity2004,newmanFastAlgorithmDetecting2004,clausetFindingCommunityStructure2004,blondelFastUnfoldingCommunities2008}. Here we consider the greedy modularity maximization algorithm  \cite{clausetFindingCommunityStructure2004}. In particular, starting with each vertex being only member of its own community, the algorithm joins the pair of communities that increases the modularity the most. This procedure is continuously conducted until no such pair exists or the termination condition is met. 

We benchmark the performance of $\QAOA$ with different partition methods in Appendix \ref{append:sec:sim-res}.


\section{\label{apx:aggregate} Proof of Theorem \ref{thm:di-an-co-maxcut}}

\begin{proof}[Proof of Theorem \ref{thm:di-an-co-maxcut}]
Although $\bx_i$ and $\bbx_i$ yield the same local objective value (i.e.,  $C_i(\bx_i)=C_i(\bbx_i)$), they may lead to a distinct  objective value (i.e.,  $C(\bx_1...\bx_i...\bx_h)\neq C(\bx_1...\bbx_i...\bx_h)$), because of the connection between $\mathsf{G}_i$ and other subgraphs $\{\mathsf{G}_{i'}\}_{i'\neq i}$. Considering that there are in total $2^h$ candidates  in $\mathcal{Z}$ in Eq.~(\ref{eqn:maxcut-divi-co}), our goal here is formulating an equivalent objective function that   finds the target bitstring satisfying
	$\hat{\bz}=\arg\min_{\bz\in \mathcal{Z}} C(\bz)$ using NISQ devices.   

Considering two neighboring subgraphs $\mathsf{G}_i$ and $\mathsf{G}_j$, we denote $\bs_i \in \{+1, -1\}$ as flipping indicator in the sense that $\bs_i=-1$ flips $\bx_i$ to be $\bbx_i$ and $\bs_i=+1$ keeps $\bx_i$ unchanged. When two subgraphs are synchronous, i.e. $\bs_i = \bs_j$, the inter-cut between them is retained and the inter-cut size is
\begin{equation}
    w_{ij}^{\text{sync}} = \sum_u \sum_v \frac{1}{2}(1-\bx_i^{(u)}\bx_j^{(v)})W_{iu,jv},
\end{equation}
where $\bx_i^{(u)}$ denotes the $u$-th bit in local solution bitstring $\bx_i$  with $u\in [|\mathsf{V}_i|]$ and $W_{iu,jv}$ is the weight of edge in $\mathsf{G}$  corresponding to two vertices $\bx_i^{(u)}$ and $\bx_j^{(v)}$.  Note that here we define $\bx_i \in \{+1,-1\}^{|V_i|}$, the coefficient $\frac{1}{2}$ acts as a standardization term to make sure $w_{ij}^{\text{sync}}$ is the sum of cut edges between $\mathsf{G}_i$ and $\mathsf{G}_j$ when adopting $\bx_i, \bx_j$ (or $\bbx_i, \bbx_j$).    When two subgraphs are asynchronous, i.e., $\bs_i = -\bs_j$, the inter-cut takes the form
\begin{equation}
    \begin{aligned}
        w_{ij}^{\text{async}} & =  \sum_u \sum_v \frac{1}{2}(1-\bx_i^{(u)}\bbx_j^{(v)})W_{iu,jv} \\
        &= \sum_u \sum_v \frac{1}{2}(1+\bx_i^{(u)}\bx_j^{(v)})W_{iu,jv},
    \end{aligned}
\end{equation}
where the second equality uses the relationship $\bbx_j = -\bx_j$.
 
 
 
Let $\bs=(\bs_1,\bs_2,\dots, \bs_h)\in \{+1,-1\}^h$ be an $h$-length bitstring as the indicator for the selection of $\{\bx_i,\bbx_i\}$ to form $\bz$. Define $w_{ij}^\prime = w_{ij}^{\text{async}}  - w_{ij}^{\text{sync}}$, $c=\sum_{i=1}^h C_i(\bx_i)+\sum_{i < j} \frac{1}{2}(w_{ij}^{\text{sync}} + w_{ij}^{\text{async}})$, and $C_i(\bx_i)$ as the local cut size of $\mathsf{G}_i$. For $\forall \bz\in\mathcal{Z}$, the objective value  $C(\bz)$ in Eq.~(\ref{eqn:maxcut-divi-co}) yields
\begin{equation}
    \label{eq:b3}
    \begin{aligned}
        &  C(\bz) \nonumber\\
        = & C(\bs_1\bx_1,\dots,\bs_h\bx_h) \nonumber\\
        = & \sum_{ i < j } \sum_u \sum_v \frac{1}{2}(1-\bs_i\bx_i^{(u)}\bs_j\bx_j^{(v)})W_{iu,jv} + \sum_i C_i(\bx_i) \nonumber\\ 
        = & \sum_{ i < j } \frac{1}{2}\left( -\bs_i \bs_j \sum_u \sum_v W_{iu,jv} \bx_i^{(u)}\bx_j^{(v)} + \sum_u \sum_v W_{iu,jv} \right) \nonumber\\ 
        & + \sum_i C_i(\bx_i) \nonumber\\
        = & \sum_{ i < j } \frac{1}{2}\left[(w_{ij}^{\text{sync}} + w_{ij}^{\text{async}}) + (w_{ij}^{\text{sync}} - w_{ij}^{\text{async}})\bs_{i}\bs_{j}\right] \nonumber\\
& + \sum_i C_i(\bx_i)\\
        = & -\frac{1}{2}\sum_{ i < j } w_{ij}^\prime \bs_i\bs_j + c,
    \end{aligned}
\end{equation}
where the second equality consists of two summations, i.e., the first term is the sum of inter cut between each pair of subgraphs and the second term is the sum of local cut inside subgraphs (which is not influenced by $\bs$),  the last second equality uses $w_{ij}^{\text{async}}  - w_{ij}^{\text{sync}} = \sum_u \sum_v W_{iu,jv} \bx_i^{(u)}\bx_j^{(v)}$ and $w_{ij}^{\text{async}}  + w_{ij}^{\text{sync}} = \sum_u \sum_v W_{iu,jv}$. 

Define $ C'(\bs) = c-\frac{1}{2}\sum_{ i < j } w_{ij}^\prime \bs_i\bs_j$. Since all local cuts $\{C_i(\bx_i)\}$  are fixed, we have
\begin{equation}
	\max_{\bs\in \{+1, -1\}^h}C'(\bs) = \max_{\bm{z}\in \mathcal{Z}}C(\bm{z}).
\end{equation} 
We find a good merging by optimizing the above.
\end{proof}


In most cases, optimizing merging of local solutions will improve the value of cut. However, if the local solutions happens to be in a good order, one can merge them without further optimization. We show the effect of merging optimization through experiments in Appendix \ref{append:sec:sim-res}.

\section{\label{apx:proof} Proof of Theorem \ref{thm:low-bound}}

\begin{proof}[Proof of Theorem \ref{thm:low-bound}]
	We follow notations defined in Appendix \ref{apx:aggregate} to prove  Theorem \ref{thm:low-bound}. Suppose that  $\mathsf{G}$ is partitioned into $h$ subgraphs $\mathsf{G}_1, \dots, \mathsf{G}_h$, we can divide edges into two parts $\mathsf{E}=\mathsf{E}^\text{inner}\bigcup \mathsf{E}^\text{inter}$ where $\mathsf{E}^\text{inner}=\bigcup_{i=1}^h \mathsf{E}_i$ denotes the set of edges inside all subgraphs and $\mathsf{E}^\text{inter}=\bigcup _{1\le i<j\le h}\mathsf{E}_{ij}$ denotes the set of edges between subgraphs.  Here the weight of edge $e\in \mathsf{E}$ is denoted   by $w(e)$. Then we have
\begin{equation}
    \label{eq:c1}
    \begin{aligned}
        \sum_{e \in \mathsf{E}} w(e) & = \sum_{e \in \mathsf{E}^\text{inner}} w(e) + \sum_{e \in \mathsf{E}^\text{inter}} w(e) \\
        & = \sum_{e\in \bigcup_{i=1}^h \mathsf{E}_i} w(e) + \sum_{e \in \bigcup _{1\le i<j\le h}\mathsf{E}_{ij}} w(e)\\
        & = \sum_{i=1}^h \sum_{e\in \mathsf{E}_i} w(e) + \sum_{1\le i<j\le h} \sum_{e\in \mathsf{E}_{ij}} w(e). 
    \end{aligned}
\end{equation}
When optimizing subgraphs, we can use any MaxCut solvers to return a set of local solutions $\{\bm{x}_i\}_{i=1}^h$ such that the cut value is greater than half of the sum of edge weights for all subgraphs $\{\mathsf{G}_i\}_{i=1}^h$. Mathematically, the sum of edge weight for each subgraph satisfies  
\begin{equation}
    \label{eq:c2}
    C_i(\bm{x}_i) \ge \frac{1}{2} \sum_{e\in \mathsf{E}_i} w(e) 
\end{equation}
and the sum of edge weight for all subgraphs yields 
\begin{equation}
    \label{eq:c3}
    C^{\text{inner}} = \sum_{i=1}^h C_i(\bm{x}_i) \ge \frac{1}{2} \sum_{e\in \bigcup_{i=1}^h \mathsf{E}_i} w(e).
\end{equation}
The above result means that we can always at least obtain half of the sum of edge weight edges inside $h$ subgraphs. 

Combining Eq.~(\ref{eq:c1}) and Eq.~(\ref{eq:c3}), an observation is that if $\QAOA$ outputs a solution $\bs=\{\bs_i\}_{i=1}^h$ such that the cut value of intra-subgraphs  achieves at least half of the second term  $\sum_{1\le i<j\le h} \sum_{e\in \mathsf{E}_{ij}} w(e)$ in Eq.~(\ref{eq:c1}), then the total cut value for the whole graph is greater than $\frac{1}{2}\sum_{e \in \mathsf{E}} w(e)$. Recall the terms $w_{ij}^{\text{async}}$ and $w_{ij}^{\text{sync}}$ defined in Theorem \ref{thm:di-an-co-maxcut}, we have 
\begin{equation}
    \label{eq:c4}
    \begin{aligned}
        \sum_{e \in \mathsf{E}^\text{inter}} w(e) &= \sum_{1\le i<j \le h} \sum_{e\in \mathsf{E}_{ij}} w(e) \\
        & = \sum_{1\le i<j \le h}  \sum_u \sum_v W_{iu,jv} \\
        & = \sum_{1\le i<j \le h} w_{ij}^{\text{async}}  + w_{ij}^{\text{sync}} \\
        & =\sum_{1\le i<j \le h} 2w_{ij}^{\text{sync}} + (w_{ij}^{\text{async}} - w_{ij}^{\text{sync}}) \\
        & = \sum_{1\le i<j \le h} 2w_{ij}^{\text{sync}} + w_{ij}^\prime \\
        & = \sum_{1\le i<j \le h} 2w_{ij}^{\text{sync}} + \sum_{1\le i<j \le h} w_{ij}^\prime.
    \end{aligned}
\end{equation}
Note that the reformulated MaxCut in Theorem \ref{thm:di-an-co-maxcut} in $\QAOA$ is 
\begin{equation}
    %\label{eq:c5}
    \begin{aligned}
        C^\prime(\bs) & = \sum_{ i < j } \frac{1}{2}\left[(w_{ij}^{\text{sync}} + w_{ij}^{\text{async}}) + (w_{ij}^{\text{sync}} - w_{ij}^{\text{async}})\bs_{i}\bs_{j}\right] \nonumber\\
        & + \sum_i C_i(\bx_i) \\
        & = \sum_{ i < j } w_{ij}^{\text{sync}} + \sum_{ i < j } (w_{ij}^{\text{async}} - w_{ij}^{\text{sync}})(1-\bs_{i}\bs_{j}) \\
        & + \sum_i C_i(\bx_i) \\
        & = \sum_{ i < j }  w_{ij}^\prime(1-\bs_{i}\bs_{j}) + \sum_{ i < j } w_{ij}^{\text{sync}} + C^{\text{inner}}
    \end{aligned}
\end{equation}
where $\sum_{ i < j }  w_{ij}^\prime(1-\bs_{i}\bs_{j})$ is optimized by the MaxCut solver and at least half of $\sum_{ i < j }  w_{ij}^\prime$ is cut. Let $\bs^*$ be the estimated solution, we have 
\begin{equation}
    \label{eq:c5}
    \begin{aligned}
        C^\prime(\bs^*) & \ge \frac{1}{2}\sum_{ i < j } w_{ij}^\prime + \sum_{ i < j } w_{ij}^{\text{sync}} + C^{\text{inner}} \\
        & = \frac{1}{2}\sum_{e \in \mathsf{E}^\text{inter}} w(e) + C^{\text{inner}} \\
        & \ge \frac{1}{2}\sum_{e \in \mathsf{E}^\text{inter}} w(e) + \frac{1}{2}\sum_{e \in \mathsf{E}^\text{inner}} w(e) \\
        & = \frac{1}{2}\sum_{e \in \mathsf{E}} w(e)
    \end{aligned}
\end{equation}
where the first inequality uses the result of reformulated MaxCut, the first equality uses the result of Eq.~(\ref{eq:c3}), the second inequality uses the result of Eq.~(\ref{eq:c4}), and the last equality uses Eq.~(\ref{eq:c1}).
\end{proof}


We end this section by illustrating when the lower bound is achieved, Consider the example of a four-vertex unweighted ring where $\mathsf{V}=\{1,2,3,4\}$ and $\mathsf{E} = \{(1,2),(2,3),(3,4),(1,4)\}$. Suppose we partition it into two subgraphs $\mathsf{G}_1$ and $\mathsf{G}_2$ with $\mathsf{V}_1=\{1,3\}$ and $\mathsf{V}_2=\{2,4\}$. So further if one of the local solutions is $(+1,-1)$, the final cut is 2 and the ratio is $\frac{1}{2}$ no matter how the global solution is merged. A smarter partition will be $\mathsf{V}_1=\{1,2\}$ and $\mathsf{V}_2=\{3,4\}$ where all edges are cut eventually. This case shows that the lower bound can be attained in worst case and effective partition strategy can alleviate this issue. 
 



\section{\label{apx:physics} Relation with Hamiltonian splitting}

Many quantum computing tasks such as QAOA or VQE aims to find an eigenstate (ground state or most excited state) corresponding to a target eigenvalue of a given Hamiltonian $H$. 
We prepare a quantum state $|\psi(\theta)\rangle$ on parameterized circuit and measure it with $H$.
Then we feed $\langle\psi(\theta)| H|\psi(\theta)\rangle$ to an optimizer and update the parameters.
To measure states on qubit quantum computers, this Hamiltonian is represented in terms of Pauli words and each Pauli word can be written as tensor product of Pauli matrices i.e. $H = \sum_k \alpha_k P_k$. 
Thus we have 
\begin{equation}
    \label{eq:e1}
    \langle\psi(\theta)| H|\psi(\theta)\rangle = \sum_k \alpha_k \langle\psi(\theta)| P_k|\psi(\theta)\rangle .
\end{equation}
This allows us to measure each Pauli words individually and add them together.
In QAOA for MaxCut problem, $H$ takes the form of \ref{eq:2} where all Pauli words are tensor products of two Pauli-Z matrices corresponding to edges in problem graph.

In general, an arbitrary Hamiltonian of $n$-qubit has $O(4^n)$ Pauli words so the query of circuit grows exponentially with qubit counts if we simply measure one Pauli word at a time.
Note that two observables can be measured simultaneously if they are commutable. Moreover, if a group of pairwise commuting observables share the same eigenbasis that diagonalizes them all simultaneously, they can be also measured on the same prepared state.
For example in MaxCut problem, all Pauli words share the same eigenbasis (computational basis) so we don't need to measure $O(n^2)$ terms individually but only once. 
One method to reduce circuit query is to split Hamiltonian into several clusters and each cluster is a commuting group of Pauli words we mentioned above.
The less number of clusters, the less of circuit query.
Recent researches mapped Hamiltonian splitting task into MinCliqueCover problem \cite{gokhaleMinimizingStatePreparations2019,verteletskyiMeasurementOptimizationVariational2020}.
Besides, additional speedup can be introduced by distributed quantum computing on multiple untangled quantum computers \cite{duAcceleratingVariationalQuantum2021}.

\begin{figure*}[h]
\centering
\includegraphics[width=0.75\textwidth]{fig5.pdf}
\caption{ \small{\textbf{Results between random and greedy partition}. } The yellow bars and green bars refer to approximation ratios of two partition strategies. The dotted line refers to the modularity of partition of each graph, which measures the quality of partitioning.}
\label{fig:5}
\end{figure*}





Nevertheless, this does not reduce the required qubit counts on quantum computer. In order to reduce qubit counts, we need to ensure that any two clusters do not share common qubit i.e. Hamiltonian of each cluster cannot be used to measure the same qubit. Since there are always Pauli words with small coefficients which play little role in final $\langle\psi(\theta)| H|\psi(\theta)\rangle$, one can pretend they don't exists when constructing circuit thus partition qubits into several parts.
If so, we can built several small and independent circuit and measure each cluster on distributed quantum computers with less qubits.
The partitioning may follow the property of primal problem such as graph weight or the mutual information between clusters \cite{zhangVariationalQuantumEigensolver2021}.
To minimize the performance loss introduced by partitioning, one can use dressed Hamiltonian in measuring \cite{zhangVariationalQuantumEigensolver2021} or include a fixing step as we discussed in Appendix \ref{apx:aggregate}.

\section{Details of numerical simulations}\label{append:sec:sim-res}

\subsection{Implementation details of $\QAOA$}
\textbf{Implementation details of $\QAOA$.}
The QAOA used in $\QAOA$ are implemented by Pennylane \cite{bergholmPennyLaneAutomaticDifferentiation2020}. In  optimization, $\QAOA$ adopts the vanilla gradient descent method to update the trainable parameters of QAOAs in which the learning rate is set as 0.01. The number of shots of each circuit is set as 1000 to approximate the expectation value of the measurements in calculating gradients. In the process of sampling solutions, $\QAOA$ runs the optimized circuit for $1000$ times to return the same number of bitstrings as solution candidates. To be more specific, in each run, all $n$ qubits are measured by the computational basis along the $Z$ direction and the measured result of each qubit is either +1 or -1. Thus an $n$-dimensional bitstring $z \in \{+1,-1\}^n$ is sampled in one query of the circuit. We collect 1000 such bitstrings as candidates and select the one as solution whose corresponding eigenvalue is the smallest among the candidates.

The source code of $\QAOA$ is available at the Github Repository \url{https://github.com/ZeddTheGoat/QAOA_in_QAQA}.

\textbf{GW algorithm.}  GW maps the primal integer programming to continuous vector space and optimizes it with semidefinte programming. The binary solution is obtained by projecting vectors to a plane. Our SDP solver is implemented by CVXPY \cite{diamondCVXPYPythonEmbeddedModeling2016}, which uses SCS (Splitting Conic Solver)  \cite{odonoghueConicOptimizationOperator2016} following conventions. Here we adopt the default parameters in executing GW, where the max number of iterations is 2500, the convergence tolerance is $1\text{e}^{-4}$,  the relaxation parameter is 1.8, and the balance between minimizing primal and dual residual is 5.0. After optimization, the solution is projected to 100 random vectors and rounded to bitstrings. 

\subsection{More simulation results of $\QAOA$}

\textbf{Performance of $\QAOA$ with the advanced partition methods.} We investigate how the advanced partition method, i.e., the greedy community detection introduced in Appendix \ref{apx:aggregate}, effects the performance of $\QAOA$. To do so, we apply $\QAOA$ to two types of graphs, i.e., w$3$e-$60$ and w$3$e-$80$. To collect the statistical results, we generate 10 instances for each setting. The allowable number of qubits is $n < 15$. The implementation of the community detection method follows the algorithm proposed in \cite{clausetFindingCommunityStructure2004}, which is realized by NetworkX library \footnote{https://github.com/networkx/networkx}. 

\begin{figure*}[h]
\centering
\includegraphics[width=0.98\textwidth]{fig6.pdf}
\caption{ \small{\textbf{Results between Naive merging and $\QAOA$}. } The yellow bars represents the approximation ratio of $\QAOA$, where solutions are merged naively. The green bars represents the approximation ratio of $\QAOA$ introduced in the main text. }
\label{fig:6}
\end{figure*}


 The simulation results are showin in  Fig.~\ref{fig:5}, where the performance of $\QAOA$ is dramatically increased when the random parition is replaced by the community detection. Specifically, for the 7-th instance of w3e-60 and the 4-th instance of w30e-80, the approximation ratio is increased by more than 0.3 compared to the random partition. Uder the measure of the modularities defined in Eq.~\ref{eq:a1}, the achieved results of $\QAOA$ with the community detection strategy are all above 0.4. This indicates good partition, which implies that most edges are kept within subgraphs. By contrast, the modularities of $\QAOA$ with the random partition strategy are about 0. These observations accord with our conjecture in main text that the cut mainly comes from within subgraphs. The advanced partition strategy, e.g., the community detection, ensures the subgraphs as dense as possible and the number of edges left between subgraphs is minimized.  


\textbf{Effect of merging optimization.} We next elucidate the importance of recasting the merging process as a new MaxCut problem used in $\QAOA$. Particularly, we conduct an ablation study by evaluating the performance of $\QAOA$ when the merging process is replaced by a naive heuristic, i.e.,  flipping non-local solutions as $\bs=\mathbf{1}$. To do so, we compare these two merging methods on u$d$r-60, u$d$r-80, u$d$e-60 and u$d$e-80 with $d\in [3,9]$ and 10 instances per setting. The number of qubits is set as $n< 10$. The collected results are shown in Fig.~\ref{fig:6}. For almost all settings, $\QAOA$ outperforms the naive approach, except for u6r-80 and u5e-80.  One possible reason is that local solutions is already good enough.   

 

\end{document}
 


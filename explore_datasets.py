#!/usr/bin/env python3
"""
Dataset Explorer for QPU vs Gurobi Benchmark Analysis
=====================================================

This script loads all benchmark datasets and provides:
1. Comprehensive structure analysis
2. Data summary statistics
3. Aggregation strategies for plotting

Data Sources (from plots_for_report.md):
- qpu_hier_repaired.json: Hierarchical QPU with post-processing repair
- gurobi_baseline_60s.json: Classical Gurobi solver (60s timeout)
- gurobi_timeout_test_300s.json: Extended Gurobi runs (300s timeout)
- qpu_benchmark_summary_*.json: Batch orchestration summaries
"""

import json
from pathlib import Path
from collections import defaultdict
from typing import Any
import glob

# ============================================================================
# Configuration
# ============================================================================

DATA_DIR = Path(__file__).parent
PRIMARY_FILES = {
    "qpu_hier": "qpu_hier_repaired.json",
    "gurobi_60s": "gurobi_baseline_60s.json",
    "gurobi_300s": "gurobi_timeout_test_300s.json",
    "qpu_hybrid": "qpu_hybrid_27food.json",
    "qpu_native": "qpu_native_results.json",
}

# ============================================================================
# Data Loading
# ============================================================================


def load_json_safe(filepath: Path) -> dict | None:
    """Load JSON file with error handling."""
    if not filepath.exists():
        return None
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            return json.load(f)
    except json.JSONDecodeError as e:
        print(f"  ‚ö†Ô∏è  JSON decode error in {filepath.name}: {e}")
        return None


def load_all_datasets() -> dict[str, dict]:
    """Load all primary datasets."""
    datasets = {}
    print("\n" + "=" * 70)
    print("üìÇ LOADING DATASETS")
    print("=" * 70)

    for key, filename in PRIMARY_FILES.items():
        filepath = DATA_DIR / filename
        data = load_json_safe(filepath)
        if data:
            datasets[key] = data
            n_runs = len(data.get("runs", []))
            print(f"  ‚úì {key:15} ‚Üí {filename:35} ({n_runs} runs)")
        else:
            print(f"  ‚úó {key:15} ‚Üí {filename:35} (not found)")

    # Load benchmark summaries
    summary_files = list(DATA_DIR.glob("qpu_benchmark_summary_*.json"))
    if summary_files:
        # Use the most recent one
        latest = max(summary_files, key=lambda p: p.stat().st_mtime)
        data = load_json_safe(latest)
        if data:
            datasets["qpu_summary"] = data
            print(f"  ‚úì {'qpu_summary':15} ‚Üí {latest.name}")

    return datasets


# ============================================================================
# Schema Analysis
# ============================================================================


def analyze_schema(data: dict, name: str) -> dict:
    """Analyze the schema of a dataset."""
    schema = {
        "name": name,
        "top_level_keys": list(data.keys()),
        "schema_version": data.get("schema_version", "unknown"),
        "generated_at": data.get("generated_at", "unknown"),
    }

    runs = data.get("runs", [])
    if runs:
        schema["n_runs"] = len(runs)
        # Sample first run for structure
        sample = runs[0]
        schema["run_keys"] = list(sample.keys())

        # Extract problem configs
        configs = []
        for run in runs:
            config = (
                run.get("n_farms", 0),
                run.get("n_foods", 0),
                run.get("n_periods", 0),
            )
            configs.append(config)
        schema["unique_configs"] = sorted(set(configs))

    return schema


def print_schema_summary(schemas: dict[str, dict]) -> None:
    """Print formatted schema summary."""
    print("\n" + "=" * 70)
    print("üìã DATASET SCHEMAS")
    print("=" * 70)

    for name, schema in schemas.items():
        print(f"\n{'‚îÄ' * 50}")
        print(f"üìä {name.upper()}")
        print(f"{'‚îÄ' * 50}")
        print(f"  Schema Version: {schema['schema_version']}")
        print(f"  Generated At:   {schema['generated_at']}")
        print(f"  Number of Runs: {schema.get('n_runs', 0)}")

        if "run_keys" in schema:
            print(f"  Run Keys ({len(schema['run_keys'])}):")
            # Group keys by category
            timing_keys = [k for k in schema["run_keys"] if "time" in k.lower()]
            objective_keys = [k for k in schema["run_keys"] if "objective" in k.lower()]
            violation_keys = [k for k in schema["run_keys"] if "violation" in k.lower()]
            config_keys = [
                k for k in schema["run_keys"] if k.startswith("n_") or k == "scenario_name"
            ]
            other_keys = [
                k
                for k in schema["run_keys"]
                if k not in timing_keys + objective_keys + violation_keys + config_keys
            ]

            print(f"    Config:     {config_keys}")
            print(f"    Timing:     {timing_keys}")
            print(f"    Objectives: {objective_keys}")
            print(f"    Violations: {violation_keys}")
            print(f"    Other:      {other_keys[:10]}{'...' if len(other_keys) > 10 else ''}")

        if "unique_configs" in schema:
            print(f"  Problem Configurations (farms √ó foods √ó periods):")
            for config in schema["unique_configs"]:
                n_vars = config[0] * config[1] * config[2]
                print(f"    {config[0]:4} √ó {config[1]:2} √ó {config[2]} = {n_vars:6} variables")


# ============================================================================
# Data Extraction & Aggregation
# ============================================================================


def extract_run_metrics(run: dict) -> dict:
    """Extract key metrics from a single run."""
    # Handle nested timing dict
    timing = run.get("timing", {})
    violations = run.get("constraint_violations", {})
    
    return {
        # Problem configuration
        "scenario": run.get("scenario_name", "unknown"),
        "n_farms": run.get("n_farms", 0),
        "n_foods": run.get("n_foods", 0),
        "n_periods": run.get("n_periods", 3),
        "n_vars": run.get("n_vars", 0),
        "formulation": "6-family" if run.get("n_foods", 0) <= 6 else "27-food",
        # Results
        "status": run.get("status", "unknown"),
        "feasible": run.get("feasible", False),
        "objective": run.get("objective_miqp"),  # Keep None to detect missing
        "mip_gap": run.get("mip_gap"),
        # Timing (all in seconds) - from nested timing dict
        "total_time": timing.get("total_wall_time", 0),
        "solve_time": timing.get("solve_time", 0),
        "qpu_time": timing.get("qpu_access_time", 0),
        "qpu_sampling": timing.get("qpu_sampling_time", 0),
        "refinement_time": timing.get("refinement_time", 0),
        "timeout": run.get("timeout_s", 60),
        # Violations - from nested constraint_violations dict
        "one_hot_violations": violations.get("one_hot_violations", 0),
        "rotation_violations": violations.get("rotation_violations", 0),
        "total_violations": violations.get("total_violations", 0),
        # Decomposition info
        "n_clusters": run.get("decomposition", {}).get("n_clusters", 1),
        # Solver info
        "mode": run.get("mode", "unknown"),
        "sampler": run.get("sampler", "unknown"),
        "backend": run.get("backend", "unknown"),
    }


def build_comparison_table(datasets: dict) -> list[dict]:
    """Build unified comparison table across all datasets."""
    rows = []

    for dataset_name, data in datasets.items():
        runs = data.get("runs", [])
        for run in runs:
            metrics = extract_run_metrics(run)
            metrics["source"] = dataset_name
            rows.append(metrics)

    return rows


def aggregate_by_scenario(rows: list[dict]) -> dict[str, dict]:
    """Aggregate metrics by scenario for cross-method comparison."""
    by_scenario = defaultdict(dict)

    for row in rows:
        scenario = row["scenario"]
        source = row["source"]

        by_scenario[scenario]["n_farms"] = row["n_farms"]
        by_scenario[scenario]["n_foods"] = row["n_foods"]
        by_scenario[scenario]["n_vars"] = row["n_vars"]
        by_scenario[scenario]["formulation"] = row["formulation"]

        # Store source-specific metrics
        by_scenario[scenario][f"{source}_objective"] = row["objective"]
        by_scenario[scenario][f"{source}_time"] = row["total_time"]
        by_scenario[scenario][f"{source}_feasible"] = row["feasible"]
        by_scenario[scenario][f"{source}_qpu_time"] = row.get("qpu_time", 0)
        by_scenario[scenario][f"{source}_violations"] = (
            row["one_hot_violations"] + row["rotation_violations"]
        )

    return dict(by_scenario)


def print_comparison_summary(aggregated: dict) -> None:
    """Print formatted comparison summary."""
    print("\n" + "=" * 70)
    print("üìä CROSS-METHOD COMPARISON")
    print("=" * 70)

    # Sort by n_vars
    sorted_scenarios = sorted(aggregated.items(), key=lambda x: x[1].get("n_vars", 0))

    print(f"\n{'Scenario':<30} {'Vars':>7} {'Form':>8} | {'Gurobi':>10} {'QPU':>10} {'Gap%':>7}")
    print("-" * 85)

    for scenario, data in sorted_scenarios:
        n_vars = data.get("n_vars", 0)
        formulation = data.get("formulation", "?")[:8]

        gurobi_obj = data.get("gurobi_60s_objective", data.get("gurobi_300s_objective"))
        qpu_obj = data.get("qpu_hier_objective", data.get("qpu_hybrid_objective"))

        gurobi_str = f"{gurobi_obj:.1f}" if gurobi_obj else "‚Äî"
        qpu_str = f"{qpu_obj:.1f}" if qpu_obj else "‚Äî"

        if gurobi_obj and qpu_obj and gurobi_obj != 0:
            gap = abs(qpu_obj - gurobi_obj) / abs(gurobi_obj) * 100
            gap_str = f"{gap:.1f}%"
        else:
            gap_str = "‚Äî"

        print(f"{scenario:<30} {n_vars:>7} {formulation:>8} | {gurobi_str:>10} {qpu_str:>10} {gap_str:>7}")


# ============================================================================
# Aggregation Strategies for Plotting
# ============================================================================


def print_aggregation_strategies(rows: list[dict]) -> None:
    """Print recommended aggregation strategies for plotting."""
    print("\n" + "=" * 70)
    print("üìà RECOMMENDED AGGREGATION STRATEGIES FOR PLOTTING")
    print("=" * 70)

    strategies = """
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STRATEGY 1: BY PROBLEM SIZE (n_vars)                                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Purpose: Scaling analysis - how does performance change with size?  ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ X-axis: n_vars (log scale)                                          ‚îÇ
‚îÇ Y-axis: Metric of interest                                          ‚îÇ
‚îÇ Grouping: By solver (Gurobi vs QPU)                                 ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ Metrics to plot:                                                    ‚îÇ
‚îÇ   ‚Ä¢ Solve time ‚Üí Shows computational scaling                        ‚îÇ
‚îÇ   ‚Ä¢ Objective value ‚Üí Shows solution quality scaling                ‚îÇ
‚îÇ   ‚Ä¢ Speedup ratio ‚Üí Shows where QPU beats classical                 ‚îÇ
‚îÇ   ‚Ä¢ Gap % ‚Üí Shows optimality degradation with scale                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STRATEGY 2: BY FORMULATION (6-family vs 27-food)                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Purpose: Compare formulation complexity effects                     ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ Split data into two formulation groups, then compare:               ‚îÇ
‚îÇ   ‚Ä¢ Time efficiency per formulation                                 ‚îÇ
‚îÇ   ‚Ä¢ QPU advantage region per formulation                            ‚îÇ
‚îÇ   ‚Ä¢ Violation rates per formulation                                 ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ Key insight: 6-family embeds better, 27-food needs decomposition    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STRATEGY 3: TIMING DECOMPOSITION                                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Purpose: Understand where time is spent                             ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ Stacked bar chart components:                                       ‚îÇ
‚îÇ   ‚Ä¢ qpu_access_time (pure quantum)                                  ‚îÇ
‚îÇ   ‚Ä¢ total_time - qpu_access_time (classical overhead)               ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ Key insight: Classical coordination dominates for small problems    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STRATEGY 4: VIOLATION IMPACT ANALYSIS                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Purpose: Correlate violations with objective gap                    ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ Scatter plot:                                                       ‚îÇ
‚îÇ   X-axis: Total violations (one_hot + rotation)                     ‚îÇ
‚îÇ   Y-axis: Gap % vs Gurobi                                           ‚îÇ
‚îÇ   Color: By formulation                                             ‚îÇ
‚îÇ   Size: By n_vars                                                   ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ Key insight: ~80-90% of gap explained by violations                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STRATEGY 5: QUANTUM ADVANTAGE HEATMAP                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Purpose: Identify advantage zones in problem space                  ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ 2D heatmap:                                                         ‚îÇ
‚îÇ   X-axis: n_farms (binned)                                          ‚îÇ
‚îÇ   Y-axis: n_foods (6 or 27)                                         ‚îÇ
‚îÇ   Color: Speedup factor or gap %                                    ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ Key insight: QPU advantage emerges for specific (farms, foods) combos‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
"""
    print(strategies)


def print_data_structure_for_plotting(rows: list[dict]) -> None:
    """Print the recommended DataFrame structure for plotting."""
    print("\n" + "=" * 70)
    print("üóÇÔ∏è  RECOMMENDED DATAFRAME STRUCTURE")
    print("=" * 70)

    structure = """
For unified analysis, create a merged DataFrame with this structure:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Column       ‚îÇ Type         ‚îÇ Description                              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ scenario     ‚îÇ str          ‚îÇ Unique problem identifier (join key)     ‚îÇ
‚îÇ n_farms      ‚îÇ int          ‚îÇ Number of farms in problem               ‚îÇ
‚îÇ n_foods      ‚îÇ int          ‚îÇ Number of foods (6 or 27)                ‚îÇ
‚îÇ n_periods    ‚îÇ int          ‚îÇ Planning periods (usually 3)             ‚îÇ
‚îÇ n_vars       ‚îÇ int          ‚îÇ Total binary variables                   ‚îÇ
‚îÇ formulation  ‚îÇ str          ‚îÇ "6-family" or "27-food"                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ gurobi_obj   ‚îÇ float        ‚îÇ Gurobi objective value                   ‚îÇ
‚îÇ gurobi_time  ‚îÇ float        ‚îÇ Gurobi solve time (s)                    ‚îÇ
‚îÇ gurobi_gap   ‚îÇ float        ‚îÇ Gurobi MIP gap if timeout                ‚îÇ
‚îÇ gurobi_status‚îÇ str          ‚îÇ "optimal" / "timeout" / "infeasible"     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ qpu_obj      ‚îÇ float        ‚îÇ QPU objective value (post-repair)        ‚îÇ
‚îÇ qpu_time     ‚îÇ float        ‚îÇ Total QPU wall time                      ‚îÇ
‚îÇ qpu_pure     ‚îÇ float        ‚îÇ Pure QPU access time                     ‚îÇ
‚îÇ qpu_feasible ‚îÇ bool         ‚îÇ Whether solution is feasible             ‚îÇ
‚îÇ violations   ‚îÇ int          ‚îÇ Total constraint violations              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ gap_pct      ‚îÇ float        ‚îÇ |qpu_obj - gurobi_obj| / |gurobi_obj| %  ‚îÇ
‚îÇ speedup      ‚îÇ float        ‚îÇ gurobi_time / qpu_time                   ‚îÇ
‚îÇ advantage    ‚îÇ bool         ‚îÇ speedup > 1 AND gap_pct < 20             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

SAMPLE CODE TO BUILD THIS:

```python
import pandas as pd

# Load datasets
with open('qpu_hier_repaired.json') as f:
    qpu_data = json.load(f)
with open('gurobi_baseline_60s.json') as f:
    gurobi_data = json.load(f)

# Build DataFrames
qpu_df = pd.DataFrame([extract_run_metrics(r) for r in qpu_data['runs']])
gurobi_df = pd.DataFrame([extract_run_metrics(r) for r in gurobi_data['runs']])

# Merge on scenario
merged = qpu_df.merge(
    gurobi_df, 
    on=['scenario', 'n_farms', 'n_foods', 'n_vars'],
    suffixes=('_qpu', '_gurobi')
)

# Compute derived metrics
merged['gap_pct'] = abs(merged['objective_qpu'] - merged['objective_gurobi']) / abs(merged['objective_gurobi']) * 100
merged['speedup'] = merged['total_time_gurobi'] / merged['total_time_qpu']
merged['advantage'] = (merged['speedup'] > 1) & (merged['gap_pct'] < 20)
```
"""
    print(structure)


# ============================================================================
# Sample Data Preview
# ============================================================================


def print_sample_data(datasets: dict) -> None:
    """Print sample data from each dataset."""
    print("\n" + "=" * 70)
    print("üîç SAMPLE DATA PREVIEW")
    print("=" * 70)

    for name, data in datasets.items():
        runs = data.get("runs", [])
        if not runs:
            continue

        print(f"\n{'‚îÄ' * 50}")
        print(f"üìä {name.upper()} - First 3 runs")
        print(f"{'‚îÄ' * 50}")

        for i, run in enumerate(runs[:3]):
            metrics = extract_run_metrics(run)
            obj_str = f"{metrics['objective']:.2f}" if metrics['objective'] is not None else "N/A"
            print(f"\n  Run {i + 1}: {metrics['scenario']}")
            print(f"    Config:    {metrics['n_farms']} farms √ó {metrics['n_foods']} foods = {metrics['n_vars']} vars")
            print(f"    Objective: {obj_str}")
            print(f"    Time:      {metrics['total_time']:.3f}s (QPU: {metrics['qpu_time']:.4f}s)")
            print(f"    Feasible:  {metrics['feasible']}")
            print(f"    Violations: {metrics['one_hot_violations']} one-hot, {metrics['rotation_violations']} rotation")


# ============================================================================
# Main Execution
# ============================================================================


def main():
    """Main entry point."""
    print("\n" + "‚ïê" * 70)
    print("  QPU vs GUROBI BENCHMARK DATA EXPLORER")
    print("  Quantum Optimization for Crop Rotation Planning")
    print("‚ïê" * 70)

    # Load all datasets
    datasets = load_all_datasets()

    if not datasets:
        print("\n‚ùå No datasets found! Check that JSON files exist.")
        return

    # Analyze schemas
    schemas = {name: analyze_schema(data, name) for name, data in datasets.items()}
    print_schema_summary(schemas)

    # Build unified comparison table
    rows = build_comparison_table(datasets)
    print(f"\nüìä Total runs across all datasets: {len(rows)}")

    # Aggregate by scenario
    aggregated = aggregate_by_scenario(rows)
    print_comparison_summary(aggregated)

    # Print sample data
    print_sample_data(datasets)

    # Print aggregation strategies
    print_aggregation_strategies(rows)
    print_data_structure_for_plotting(rows)

    # Summary statistics
    print("\n" + "=" * 70)
    print("üìà SUMMARY STATISTICS")
    print("=" * 70)

    sources = defaultdict(list)
    for row in rows:
        sources[row["source"]].append(row)

    for source, source_rows in sources.items():
        n_feasible = sum(1 for r in source_rows if r["feasible"])
        avg_time = sum(r["total_time"] for r in source_rows) / len(source_rows)
        var_range = (
            min(r["n_vars"] for r in source_rows),
            max(r["n_vars"] for r in source_rows),
        )

        print(f"\n  {source.upper()}:")
        print(f"    Runs:        {len(source_rows)}")
        print(f"    Feasible:    {n_feasible}/{len(source_rows)} ({100*n_feasible/len(source_rows):.0f}%)")
        print(f"    Avg Time:    {avg_time:.3f}s")
        print(f"    Var Range:   {var_range[0]} - {var_range[1]}")


if __name__ == "__main__":
    main()

# Instruction Prompt: Unify LQ Benchmark JSON Output

## 1. High-Level Goal

The primary objective is to refactor the `benchmark_scalability_LQ.py` script so that its JSON outputs have the **exact same structure** as those generated by `comprehensive_benchmark.py`. This will standardize the data format, making cross-benchmark analysis and plotting much simpler.

This involves changing the structure from a flat dictionary of results per run to a nested dictionary organized by scenario and solver.

## 2. Target JSON Structure Analysis

First, analyze the target output format from `comprehensive_benchmark.py`. A single sample result from its output file looks like this:

```json
{
  "sample_id": 0,
  "scenario_type": "farm",
  "n_units": 25,
  "total_area": 117.21,
  "n_foods": 27,
  "n_variables": 1350,
  "n_constraints": 1625,
  "cqm_time": 0.613,
  "solvers": {
    "gurobi": {
      "status": "Optimal",
      "objective_value": 0.4299,
      "solve_time": 0.107,
      "solver_time": 0.049,
      "success": true,
      "solution_areas": { "...": "..." }
    },
    "dwave_cqm": {
      "status": "Optimal",
      "objective_value": 65.07,
      "solve_time": 5.332,
      "qpu_time": 0.069,
      "hybrid_time": 5.332,
      "success": true,
      "solution_summary": { "...": "..." },
      "validation": { "...": "..." }
    }
  }
}
```

**Key Structural Elements to Replicate:**
1.  A main dictionary for each run/sample.
2.  A nested `solvers` dictionary containing the results for each solver.
3.  Consistent naming for solvers (`gurobi` for the main classical solver, `dwave_cqm` for the D-Wave CQM solver).
4.  Consistent naming for timing fields (`solve_time`, `solver_time`, `qpu_time`, `hybrid_time`).
5.  Inclusion of a `solution_summary` and a `validation` block within each solver's results.

## 3. Core Task: Modify `benchmark_scalability_LQ.py`

The main changes will be in the `run_benchmark` and `main` functions of this script.

### 3.1. Refactor the `run_benchmark` Function

This function currently returns a flat dictionary. It must be modified to return a nested dictionary matching the target structure.

**Instructions:**

1.  **Change the Return Structure:** Modify `run_benchmark` to build and return a dictionary that starts with top-level keys:
    *   `sample_id` (use `run_number`)
    *   `scenario_type` (hardcode this to `"lq_farm"`)
    *   `n_units` (use `n_farms`)
    *   `total_area` (calculate from `config`)
    *   `n_foods`, `n_variables`, `n_constraints`, `cqm_time`, etc.
    *   A `solvers` dictionary.

2.  **Populate the `solvers` Dictionary:** Move the results for each solver into the `solvers` dictionary under the correct key.

    *   **PuLP Results:**
        *   The results from `solve_with_pulp` should be placed under the key `"gurobi"`. This is for consistency with `comprehensive_benchmark`, which uses Gurobi via PuLP.
        *   The `pulp_time` should be renamed to `solve_time`.
        *   The `pulp_results` dictionary should be augmented to include a `solver_time` key (if available) and a `solution_summary`.
        *   Ensure the `validation` block from `pulp_results` is included.

    *   **Pyomo Results:**
        *   The results from `solve_with_pyomo` should be placed under a new key, for example, `"pyomo_native"`.
        *   The `pyomo_time` should be renamed to `solve_time`.
        *   Ensure its result dictionary includes `solver_time`, `solution_summary`, and a `validation` block.

    *   **D-Wave Results:**
        *   The D-Wave results should be placed under the key `"dwave_cqm"`.
        *   The main `dwave_time` should be renamed to `solve_time`.
        *   The `hybrid_time` and `qpu_time` fields should be preserved as they are.
        *   Ensure a `solution_summary` and `validation` block are generated and included.

### 3.2. Refactor the `main` Function

The `main` function orchestrates the runs and saves the final files.

**Instructions:**

1.  **Update Result Aggregation:** The logic that aggregates results from multiple runs needs to be updated to handle the new nested dictionary structure.
2.  **Standardize Final JSON Output:**
    *   The script currently saves two files: `benchmark_lq_all_runs_...` and `benchmark_lq_aggregated_...`.
    *   The `all_runs` file should now be a single JSON object with a `metadata` key and a `lq_results` key (containing a list of the newly formatted result dictionaries), mirroring the structure of `comprehensive_benchmark.py`.
    *   The `aggregated` file can remain as is for now, as it's for plotting, but the "all runs" file is the priority for standardization.

## 4. Core Task: Modify `solver_runner_LQ.py`

To support the changes above, the solver functions in this script must be updated to return all the necessary information, particularly the `solution_summary` and `validation` blocks.

### 4.1. Add `solution_summary` Generation

1.  **Create a `extract_solution_summary` function:** You can copy the logic from `solver_runner_BINARY.py`'s `extract_solution_summary` function. You will need to adapt it for the `LQ` formulation, which uses both `A` (area) and `Y` (selection) variables. The summary should include crops selected, total area per crop, and the farms assigned.
2.  **Call this function** inside `solve_with_pulp` and `solve_with_pyomo` after a solution is found, and add the returned summary to the results dictionary.

### 4.2. Add Constraint Validation

1.  **Use the existing `validate_solution_constraints` function:** This function is already present in `solver_runner_LQ.py`.
2.  **Call this function** inside `solve_with_pulp` and `solve_with_pyomo`.
3.  **Important:** The `solution` dictionary passed to the validation function must contain both the area (`A`) and selection (`Y`) variables. You may need to merge the `areas` and `selections` dictionaries before calling the validation function.
4.  Add the returned `validation` dictionary to the results dictionary for each solver.

By following these steps, the `benchmark_scalability_LQ.py` script will produce JSON files that are structurally consistent with `comprehensive_benchmark.py`, enabling unified data processing and analysis.
